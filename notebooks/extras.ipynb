{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Extras: Sequence Models and Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional references: \n",
    "- [Character Level Language Model (GPU required)](https://github.com/m2dsupsdlclass/lectures-labs/blob/master/labs/06_deep_nlp/Character_Level_Language_Model_rendered.ipynb)\n",
    "- [Transformers (BERT fine-tuning): Joint Intent Classification and Slot Filling](https://github.com/m2dsupsdlclass/lectures-labs/blob/master/labs/06_deep_nlp/Transformers_Joint_Intent_Classification_Slot_Filling_rendered.ipynb)\n",
    "- [Generating Language with huggingface](https://huggingface.co/blog/how-to-generate)\n",
    "- [huggingface examples](https://huggingface.co/transformers/quickstart.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T16:28:40.562418Z",
     "start_time": "2022-05-05T16:28:40.194542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 768 entries, 0 to 819\n",
      "Data columns (total 14 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   case_name       768 non-null    object        \n",
      " 1   opinion_type    768 non-null    object        \n",
      " 2   date_standard   768 non-null    datetime64[ns]\n",
      " 3   authorship      768 non-null    object        \n",
      " 4   x_republican    768 non-null    float64       \n",
      " 5   maj_judges      768 non-null    object        \n",
      " 6   dissent_judges  768 non-null    object        \n",
      " 7   topic_id        768 non-null    float64       \n",
      " 8   cite_count      768 non-null    float64       \n",
      " 9   opinion_text    768 non-null    object        \n",
      " 10  year            768 non-null    int64         \n",
      " 11  log_cite_count  768 non-null    float64       \n",
      " 12  preprocessed    768 non-null    object        \n",
      " 13  author_id       768 non-null    int8          \n",
      "dtypes: datetime64[ns](1), float64(4), int64(1), int8(1), object(7)\n",
      "memory usage: 84.8+ KB\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load data from a pickle file (assuming it's a Pandas DataFrame)\n",
    "df = pd.read_pickle('sc_cases_cleaned.pkl', compression='gzip')\n",
    "df = df.assign(author_id=(df['authorship']).astype('category').cat.codes)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T06:30:57.665050Z",
     "start_time": "2022-04-01T06:30:57.645799Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encode the 'author_id' column using LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "J = encoder.fit_transform(df['author_id'].astype(str))\n",
    "num_judges = max(J) + 1\n",
    "\n",
    "# Define the target variables Y and Y2\n",
    "Y = (df['x_republican'] > 0).astype(int)\n",
    "Y2 = df['log_cite_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "778e1864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 10,  5,  4,  4,  9,  1,  3,  2,  3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b769d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a PyTorch model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_judges):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_judges, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(2, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1c6bc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmarangon/anaconda3/envs/nlp_env_newsp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding Embedding(11, 2)\n",
      "flatten Flatten(start_dim=1, end_dim=-1)\n",
      "fc1 Linear(in_features=2, out_features=2, bias=True)\n",
      "fc2 Linear(in_features=2, out_features=1, bias=True)\n",
      "sigmoid Sigmoid()\n"
     ]
    }
   ],
   "source": [
    "# Initiate the model\n",
    "model = NeuralNetwork(num_judges)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Print the PyTorch model summary\n",
    "for name, layer in model.named_children():\n",
    "    print(name, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81952aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(J, dtype=torch.long)\n",
    "Y_torch = torch.tensor(Y.to_numpy(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f518578e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0:\n",
      "---- Training loss: 0.58, validation loss: 0.57\n",
      "---- Training accuracy: 0.77, validation accuracy: 0.81\n",
      "Finished epoch 10:\n",
      "---- Training loss: 0.45, validation loss: 0.43\n",
      "---- Training accuracy: 0.77, validation accuracy: 0.81\n",
      "Finished epoch 20:\n",
      "---- Training loss: 0.23, validation loss: 0.23\n",
      "---- Training accuracy: 1.0, validation accuracy: 1.0\n",
      "Finished epoch 30:\n",
      "---- Training loss: 0.08, validation loss: 0.09\n",
      "---- Training accuracy: 1.0, validation accuracy: 1.0\n",
      "Finished epoch 40:\n",
      "---- Training loss: 0.03, validation loss: 0.04\n",
      "---- Training accuracy: 1.0, validation accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_torch, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# create objects for logging\n",
    "history_val_loss = []\n",
    "history_train_loss = []\n",
    "history_val_acc = []\n",
    "history_train_acc = []\n",
    "\n",
    "# hold the best model\n",
    "best_loss = np.inf   # init to infinity\n",
    "best_weights = None\n",
    "best_epoch = 0\n",
    "\n",
    "# set main parameters for training\n",
    "n_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # take a batch\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        # forward pass\n",
    "        y_pred = model(X_batch)\n",
    "        y_batch = y_batch.unsqueeze(1)\n",
    "        loss = loss_function(y_pred, y_batch)\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # evaluate model at end of each epoch\n",
    "    model.eval()\n",
    "    \n",
    "    y_pred_train_probs = model(X_train)\n",
    "    y_pred_train = torch.where(y_pred_train_probs > 0.5, 1, 0)\n",
    "\n",
    "    y_pred_val_probs = model(X_val)\n",
    "    y_pred_val = torch.where(y_pred_val_probs > 0.5, 1, 0)\n",
    "    \n",
    "    y_train_unsq = y_train.unsqueeze(1)\n",
    "    loss_train = loss_function(y_pred_train_probs, y_train_unsq)\n",
    "    loss_train = float(loss_train)\n",
    "    history_train_loss.append(loss_train)\n",
    "    acc_train = accuracy_score(y_pred_train, y_train)\n",
    "    history_train_acc.append(acc_train)\n",
    "\n",
    "    y_val_unsq = y_val.unsqueeze(1)\n",
    "    loss_val = loss_function(y_pred_val_probs, y_val_unsq)\n",
    "    loss_val = float(loss_val)\n",
    "    history_val_loss.append(loss_val)\n",
    "    acc_val = accuracy_score(y_pred_val, y_val)\n",
    "    history_val_acc.append(acc_val)\n",
    "\n",
    "    if loss_val < best_loss:\n",
    "        best_loss = loss_val\n",
    "        best_epoch = epoch\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # print some information\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Finished epoch {epoch}:')\n",
    "        print(f'---- Training loss: {np.round(loss_train, 2)}, validation loss: {np.round(loss_val, 2)}')\n",
    "        print(f'---- Training accuracy: {np.round(acc_train, 2)}, validation accuracy: {np.round(acc_val, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T06:31:03.019931Z",
     "start_time": "2022-04-01T06:31:00.195438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 599.472x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 599.472x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 599.472x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 599.472x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 599.472x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize the judge vectors\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "judge_cites = dict(Y2.groupby(J).mean())\n",
    "rep = dict(Y.groupby(J).mean())\n",
    "df2 = pd.DataFrame(J,columns=['authorship']).drop_duplicates().sort_values('authorship')\n",
    "df2['cites'] = df2['authorship'].apply(lambda x: judge_cites[x])\n",
    "df2['republican'] = df2['authorship'].apply(lambda x: rep[x])\n",
    "\n",
    "#define new optimizer and loss function\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for i in range(5):\n",
    "    if i > 0:\n",
    "        for epoch in range(1):\n",
    "            model.train()\n",
    "            judge_vec = torch.tensor(J)\n",
    "            Y2_torch = torch.tensor(Y2, dtype=torch.float32).reshape(-1, 1)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(judge_vec)\n",
    "            loss = loss_function(output, Y2_torch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Judge vectors (assuming 'model' contains the embedding layer)\n",
    "    embedding_weights = model.embedding.weight.data.numpy()\n",
    "    df2['x'] = embedding_weights[:, 0]\n",
    "    df2['y'] = embedding_weights[:, 1]\n",
    "    \n",
    "    # Visualization\n",
    "    sns.relplot(data=df2, x=\"x\", y=\"y\", hue='republican', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T16:28:43.360106Z",
     "start_time": "2022-05-05T16:28:42.904969Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "# gpu or cpu?\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model from a pretrained checkpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T16:29:04.222014Z",
     "start_time": "2022-05-05T16:28:45.142458Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased' # huggingface model_ID or path to folder \n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T16:29:46.864883Z",
     "start_time": "2022-05-05T16:29:42.989169Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4669 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "inputs = tokenizer(df.iloc[0]['opinion_text'], return_tensors=\"pt\")\n",
    "#[32, 100, 23, 0, 0, 0, 0, 0] -> [1, 1, 1, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T16:45:38.818606Z",
     "start_time": "2022-05-05T16:45:37.127328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  3425, 18353,  ...,  6525,  3089,   102],\n",
      "        [  101,  3425,  8799,  ...,  4781,  2580,   102],\n",
      "        [  101,  3425,  1051,  ..., 13931,  9964,   102],\n",
      "        ...,\n",
      "        [  101,  3425,  8040,  ...,  2005,  1996,   102],\n",
      "        [  101,  3425,  2726,  ...,  2015,  2006,   102],\n",
      "        [  101,  3425,  1051,  ..., 25394, 11461,   102]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])} tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
      "        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(df['opinion_text'].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "labels = torch.tensor(df['x_republican'].tolist()).long() \n",
    "print(inputs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More infos about huggingface tokenizers can be found [here](https://huggingface.co/transformers/main_classes/tokenizer.html).\n",
    "\n",
    "Now we have a set of text inputs and authors indicators as labels and we can train a transformers model using a cross-entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T16:45:40.928552Z",
     "start_time": "2022-05-05T16:45:38.819643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [174 594]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "unique_labels, counts = np.unique(df[\"x_republican\"], return_counts=True)\n",
    "print (unique_labels, counts)\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(unique_labels))\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.distilbert.parameters(), 'lr': 1e-5},  \n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T16:45:41.393502Z",
     "start_time": "2022-05-05T16:45:40.929799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(608,) (152,) (608,) (152,)\n",
      "(76, 8) (19, 8) (76, 8) (19, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['opinion_text'].tolist(), df['x_republican'].tolist(), test_size=.2)\n",
    "\n",
    "# generate batches\n",
    "X_train, X_test, y_train, y_test = np.array(X_train[:608]), np.array(X_test[:152]), np.array(y_train[:608]), np.array(y_test[:152])\n",
    "print (X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X_train.reshape(-1, 8), X_test.reshape(-1, 8), y_train.reshape(-1, 8), y_test.reshape(-1, 8)\n",
    "print (X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "X_train, X_test = X_train.tolist(), X_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T16:47:09.918297Z",
     "start_time": "2022-05-05T16:45:42.809048Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 76/76 [05:24<00:00,  4.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for text, labels in tqdm(zip(X_train, y_train), total=len(X_train)):\n",
    "        # prepare model input through our tokenizer\n",
    "        model_inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "        # place everything on the right device\n",
    "        model_inputs = {k:v.to(device) for k,v in model_inputs.items()}\n",
    "        # labels have to be torch long tensors\n",
    "        labels = torch.tensor(labels).long()\n",
    "        # now, we can perform the forward pass\n",
    "        output = model(**model_inputs, labels=labels)\n",
    "        loss, logits = output[:2]\n",
    "        # and the backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T16:47:46.266853Z",
     "start_time": "2022-05-05T16:47:32.428236Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 19/19 [00:48<00:00,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        27\n",
      "         1.0       1.00      1.00      1.00       125\n",
      "\n",
      "    accuracy                           1.00       152\n",
      "   macro avg       1.00      1.00      1.00       152\n",
      "weighted avg       1.00      1.00      1.00       152\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions, targets = [], []\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text, labels in tqdm(zip(X_test, y_test), total=len(X_test)):\n",
    "        model_inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        model_inputs = {k:v.to(device) for k,v in model_inputs.items()}\n",
    "\n",
    "        output = model(**model_inputs)\n",
    "        logits = output[0]\n",
    "        # prediction is the argmax of the logits\n",
    "        predictions.extend(logits.argmax(dim=1).tolist())\n",
    "        targets.extend(labels)\n",
    "        \n",
    "from sklearn import metrics\n",
    "accuracy = metrics.accuracy_score(targets, predictions)\n",
    "print (\"accuracy\", accuracy)\n",
    "classification_report = metrics.classification_report(targets, predictions)\n",
    "print (classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we considered the pytorch version for transformers. It also works with keras, a more in-depth tutorial can be found [here](https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T16:51:32.596448Z",
     "start_time": "2022-05-05T16:51:01.740495Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 20:17:33.626499: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-31 20:17:34.675056: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-31 20:17:34.675075: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-10-31 20:17:35.929498: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-31 20:17:35.929857: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-31 20:17:35.929873: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-10-31 20:17:38.483166: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-10-31 20:17:38.483550: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-10-31 20:17:38.483631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (led-l03): /proc/driver/nvidia/version does not exist\n",
      "2023-10-31 20:17:38.485168: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-31 20:17:39.348940: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2023-10-31 20:17:39.471842: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2023-10-31 20:17:39.487033: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2023-10-31 20:17:40.620201: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cmarangon/anaconda3/envs/nlp_env_newsp/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, DistilBertConfig\n",
    "import tensorflow as tf\n",
    "\n",
    "# note that we use TFDistilBert... instead of DistilBert...\n",
    "\n",
    "transformer_model = TFDistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# define model input layer\n",
    "\n",
    "input_ids = tf.keras.layers.Input(shape=(256,), name='input_token', dtype='int32')\n",
    "input_masks_ids = tf.keras.layers.Input(shape=(256,), name='masked_token', dtype='int32')\n",
    "X = transformer_model(input_ids, input_masks_ids)\n",
    "model = tf.keras.Model(inputs=[input_ids, input_masks_ids], outputs = X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T16:51:32.605055Z",
     "start_time": "2022-05-05T16:51:32.597485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_token (InputLayer)       [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " masked_token (InputLayer)      [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_for_sequence_cl  TFSequenceClassifie  66955010   ['input_token[0][0]',            \n",
      " assification (TFDistilBertForS  rOutput(loss=None,               'masked_token[0][0]']           \n",
      " equenceClassification)         logits=(None, 2),                                                 \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,955,010\n",
      "Trainable params: 66,955,010\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T16:54:06.621809Z",
     "start_time": "2022-05-05T16:54:06.593204Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', # cost function\n",
    "              optimizer='adam', # use adam as the optimizer\n",
    "              metrics=['accuracy']) # compute accuracy, for scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T16:54:15.329792Z",
     "start_time": "2022-05-05T16:54:07.325002Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize X_train\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['opinion_text'].tolist(), df['x_republican'].tolist(), test_size=.2)\n",
    "X_train_tf = [tokenizer(x, return_tensors=\"tf\", padding=True, truncation=True, max_length=256) for x in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T16:54:15.427831Z",
     "start_time": "2022-05-05T16:54:15.330762Z"
    }
   },
   "outputs": [],
   "source": [
    "input_ids, input_masks = [x[\"input_ids\"][0].numpy() for x in X_train_tf], [x[\"attention_mask\"][0].numpy() for x in X_train_tf]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({'input_token': input_ids, 'masked_token': input_masks}, y_train)).batch(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T16:54:48.434829Z",
     "start_time": "2022-05-05T16:54:15.428719Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 20:18:11.416991: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_info = model.fit(dataset,epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77dd65",
   "metadata": {},
   "source": [
    "# Deep Learning - Extra Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da12955",
   "metadata": {},
   "source": [
    "During backpropagation the algorithm computes the gradient of the cost function w.r.t each parameter and it uses these gradients to update each parameter with a Gradient Descent step. <br>\n",
    "\n",
    "<b>Vanishing gradient problem:</b> gradients often get smaller and smaller as the algorithm progresses down to the lower layers $\\implies$ lower layers connection weights remain virtually unchanged. <br>\n",
    "\n",
    "<b>Exploding gradient problem:</b> gradients grow bigger and bigger until layers get insanely large weight updates and the algorithm diverges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5020c7",
   "metadata": {},
   "source": [
    "![attachment:image.png](https://files.codingninjas.in/article_images/vanishing-and-exploding-gradients-0-1644853457.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161b421",
   "metadata": {},
   "source": [
    "### Initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91a4041",
   "metadata": {},
   "source": [
    "**Problems:**\n",
    "\n",
    "- Initial weights too large $\\implies$ Exploding Gradient\n",
    "- Initial weights too small $\\implies$ Vanishing Gradient\n",
    "- Correlated (collinear) initial weights $\\implies$ Backpropagation cannot distinguish their contribution to the output error\n",
    "\n",
    "**Solution:** Initialization of neuron weights should be random *but* you can choose the distribution (standard practice is to use \"Glorot\" and \"HeNormal\", which alleviate the problem of unstable gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0725fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "raw_data = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2fc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  \n",
       "0    -122.23  \n",
       "1    -122.22  \n",
       "2    -122.24  \n",
       "3    -122.25  \n",
       "4    -122.25  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# re-load the housing data\n",
    "df = pd.DataFrame(raw_data.data, columns=raw_data['feature_names'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1485fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-load the housing data\n",
    "df = pd.DataFrame(raw_data.data, columns=raw_data['feature_names'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109f26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.iloc[:,:], raw_data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# before bulding the model we will transform all our data to tensors\n",
    "X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "X_val = torch.tensor(X_val.to_numpy(), dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self, X):\n",
    "      super().__init__()\n",
    "      # define all the components of the NN\n",
    "      self.linear1 = nn.Linear(in_features=X.shape[1], out_features=50)\n",
    "      torch.nn.init.xavier_uniform_(self.linear1.weight) # Glorot initialization! \n",
    "      self.act1 = nn.ReLU() \n",
    "      self.linear2 = nn.Linear(in_features=50, out_features=1)\n",
    "      self.act2 = nn.Sigmoid()\n",
    "\n",
    "    # define how to transform the input\n",
    "    def forward(self, X):\n",
    "        X = self.act1(self.linear1(X))\n",
    "        output = self.act2(self.linear2(X))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = MyNeuralNetwork(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83dd092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3165,  0.0996, -0.3036,  0.0872,  0.0322, -0.1303, -0.1353, -0.0381],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# inspect the initialized weigths\n",
    "model.linear1.weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7511e9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16512, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# can our model handle our data?\n",
    "random_results = model.forward(X_train)\n",
    "random_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1d66e",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07216af",
   "metadata": {},
   "source": [
    "**Problem:** Distribution of weights is likely to change at every step of backpropagation $\\implies$ slower and less efficient model.\n",
    "\n",
    "**Solution:** after each layer normalize the inputs to have mean zero and variance 1. It helps to increase performance, speed up training, and regularize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d3597",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self, X):\n",
    "      super().__init__()\n",
    "      # define all the components of the NN\n",
    "      self.linear1 = nn.Linear(in_features=X.shape[1], out_features=50)\n",
    "      torch.nn.init.xavier_uniform_(self.linear1.weight) # Glorot initialization!\n",
    "      self.normalize1 = nn.BatchNorm1d(50)   # apply batch normalization\n",
    "      self.act1 = nn.ReLU()\n",
    "      self.linear2 = nn.Linear(in_features=50, out_features=1)\n",
    "      self.act2 = nn.Sigmoid()\n",
    "\n",
    "    # define how to transform the input\n",
    "    def forward(self, X):\n",
    "        X = self.act1(self.linear1(X))\n",
    "        X_normalized = self.normalize1(X)\n",
    "        output = self.act2(self.linear2(X_normalized))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d433c455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16512, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the model\n",
    "model = MyNeuralNetwork(X_train)\n",
    "\n",
    "# can our model handle our data?\n",
    "random_results = model.forward(X_train)\n",
    "random_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c415d1ac",
   "metadata": {},
   "source": [
    "### Alternative Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a9ed3",
   "metadata": {},
   "source": [
    "![attachment:image.png](https://www.researchgate.net/profile/Alberto-Marchisio/publication/328878703/figure/fig2/AS:691988286423047@1541994272922/Behavior-of-ReLU-ELU-and-SELU-activation-functions.ppm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc09eb",
   "metadata": {},
   "source": [
    "Exponential Linear Unit (ELU) and Scaled ELU (SELU). In particular, SELU enhances the performance of the network, that is a neural network with only dense layers, each using SELU as activation function, the network will self-normalize, provided that:\n",
    "- inputs are standardized\n",
    "- it uses LeCun normal initialization for weights\n",
    "- set $\\lambda \\approx 1.0507$ and $\\alpha \\approx 1.6732$\n",
    "- in general SELU > ELU > ReLU\n",
    "\n",
    "Note that with SELU batch normalization is not needed as the newtwork self-normalizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550640f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SELU()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.ELU(alpha=1)\n",
    "nn.SELU()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4329c6d2",
   "metadata": {},
   "source": [
    "### Optimizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536c0c7e",
   "metadata": {},
   "source": [
    "Existing research showed that it has a big impact on the performance of the model. <i>Adam</i> (adaptive moment estimation) is faster and almost as good as <i>sgd</i>. But there are multiple options available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da1b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer_adam = optim.Adam(model.parameters(), lr=0.0001)\n",
    "optimizer_adamw = optim.AdamW(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c24b2cd",
   "metadata": {},
   "source": [
    "### Regularization and Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391adf11",
   "metadata": {},
   "source": [
    "Neural network parameters, like regressions, can be regularized with L1 and/or L2 penalties to push weak neurons to zero and create a sparse model $\\rightarrow$ avoid overfitting.\n",
    "\n",
    "In PyTorch, a simple way to add regularization is through the weight decay parameter of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cee2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_adam = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac56b4e",
   "metadata": {},
   "source": [
    "Alternatively, use <i>dropout</i> during training (it is NOT used during validation), with probability ($p$) between 10% and 50%. Alternative dropout regularizations are:\n",
    "- Normal dropout: neurons don't get dropped but coefficients are down-weighted by $p$\n",
    "- Monte Carlo dropout: continues to allow dropout but produce 100 predictions, and averages them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa5100",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self, X):\n",
    "      super().__init__()\n",
    "      self.linear1 = nn.Linear(in_features=X.shape[1], out_features=50)\n",
    "      torch.nn.init.xavier_uniform_(self.linear1.weight) # Glorot initialization\n",
    "      self.normalize1 = nn.BatchNorm1d(50)   # apply batch normalization\n",
    "      self.dropout1 = nn.Dropout(p=0.3)\n",
    "      self.act1 = nn.ReLU()\n",
    "      self.linear2 = nn.Linear(in_features=50, out_features=1)\n",
    "      self.act2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.act1(self.linear1(X))\n",
    "        X_normalized = self.normalize1(X)\n",
    "        X = self.dropout1(X)\n",
    "        output = self.act2(self.linear2(X_normalized))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dcd130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16512, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# can our model handle our data?\n",
    "random_results = model.forward(X_train)\n",
    "random_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c7083",
   "metadata": {},
   "source": [
    "## Tuning NN Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd94b96",
   "metadata": {},
   "source": [
    "It is an active area of research. One option is to try combinations of hyperparameters and see which one works best on the validation set, using GridSearchCV or RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e1f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install first\n",
    "# pip install skorch\n",
    "from skorch import NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self, X, n_neurons):\n",
    "      super().__init__()\n",
    "      self.linear1 = nn.Linear(in_features=X.shape[1], out_features=n_neurons)\n",
    "      torch.nn.init.xavier_uniform_(self.linear1.weight) # Glorot initialization\n",
    "      self.normalize1 = nn.BatchNorm1d(n_neurons)   # apply batch normalization\n",
    "      self.dropout1 = nn.Dropout(p=0.3)\n",
    "      self.act1 = nn.ReLU()\n",
    "      self.linear2 = nn.Linear(in_features=n_neurons, out_features=1)\n",
    "      self.act2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.act1(self.linear1(X))\n",
    "        X_normalized = self.normalize1(X)\n",
    "        X = self.dropout1(X)\n",
    "        output = self.act2(self.linear2(X_normalized))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c7fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the skorch wrapper\n",
    "model = NeuralNet(\n",
    "    module=MyNeuralNetwork(X_train, n_neurons=50),  # neural network architecture\n",
    "    criterion=nn.MSELoss()   # loss function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2d9bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'skorch.net.NeuralNet'>[initialized](\n",
      "  module_=MyNeuralNetwork(\n",
      "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
      "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout1): Dropout(p=0.3, inplace=False)\n",
      "    (act1): ReLU()\n",
      "    (linear2): Linear(in_features=50, out_features=1, bias=True)\n",
      "    (act2): Sigmoid()\n",
      "  ),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.initialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e25790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9703479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=&lt;class &#x27;skorch.net.NeuralNet&#x27;&gt;[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)\n",
       "    (...\n",
       "        [   3.8125,   49.0000,    4.4735,  ...,    1.7381,   33.7700,\n",
       "         -118.1600],\n",
       "        [   4.1563,    4.0000,    5.6458,  ...,    2.7232,   34.6600,\n",
       "         -120.4800],\n",
       "        ...,\n",
       "        [   2.9344,   36.0000,    3.9867,  ...,    3.3321,   34.0300,\n",
       "         -118.3800],\n",
       "        [   5.7192,   15.0000,    6.3953,  ...,    3.1789,   37.5800,\n",
       "         -121.9600],\n",
       "        [   2.5755,   52.0000,    3.4026,  ...,    2.1087,   37.7700,\n",
       "         -122.4200]])],\n",
       "                         &#x27;module__n_neurons&#x27;: [10, 50, 100]},\n",
       "             refit=False, scoring=&#x27;neg_mean_squared_error&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=&lt;class &#x27;skorch.net.NeuralNet&#x27;&gt;[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)\n",
       "    (...\n",
       "        [   3.8125,   49.0000,    4.4735,  ...,    1.7381,   33.7700,\n",
       "         -118.1600],\n",
       "        [   4.1563,    4.0000,    5.6458,  ...,    2.7232,   34.6600,\n",
       "         -120.4800],\n",
       "        ...,\n",
       "        [   2.9344,   36.0000,    3.9867,  ...,    3.3321,   34.0300,\n",
       "         -118.3800],\n",
       "        [   5.7192,   15.0000,    6.3953,  ...,    3.1789,   37.5800,\n",
       "         -121.9600],\n",
       "        [   2.5755,   52.0000,    3.4026,  ...,    2.1087,   37.7700,\n",
       "         -122.4200]])],\n",
       "                         &#x27;module__n_neurons&#x27;: [10, 50, 100]},\n",
       "             refit=False, scoring=&#x27;neg_mean_squared_error&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: NeuralNet</label><div class=\"sk-toggleable__content\"><pre>&lt;class &#x27;skorch.net.NeuralNet&#x27;&gt;[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)\n",
       "    (act2): Sigmoid()\n",
       "  ),\n",
       ")</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NeuralNet</label><div class=\"sk-toggleable__content\"><pre>&lt;class &#x27;skorch.net.NeuralNet&#x27;&gt;[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)\n",
       "    (act2): Sigmoid()\n",
       "  ),\n",
       ")</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=<class 'skorch.net.NeuralNet'>[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)\n",
       "    (...\n",
       "        [   3.8125,   49.0000,    4.4735,  ...,    1.7381,   33.7700,\n",
       "         -118.1600],\n",
       "        [   4.1563,    4.0000,    5.6458,  ...,    2.7232,   34.6600,\n",
       "         -120.4800],\n",
       "        ...,\n",
       "        [   2.9344,   36.0000,    3.9867,  ...,    3.3321,   34.0300,\n",
       "         -118.3800],\n",
       "        [   5.7192,   15.0000,    6.3953,  ...,    3.1789,   37.5800,\n",
       "         -121.9600],\n",
       "        [   2.5755,   52.0000,    3.4026,  ...,    2.1087,   37.7700,\n",
       "         -122.4200]])],\n",
       "                         'module__n_neurons': [10, 50, 100]},\n",
       "             refit=False, scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    'lr': [0.01, 0.02],\n",
    "    'max_epochs': [10, 20],\n",
    "    \"module__X\": [X_train],\n",
    "    'module__n_neurons': [10, 50, 100],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(model, params, refit=False, cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
    "gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7bee59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6349\u001b[0m        \u001b[32m3.2197\u001b[0m  0.4013\n",
      "      2        \u001b[36m3.1818\u001b[0m        \u001b[32m2.8635\u001b[0m  0.3041\n",
      "      3        \u001b[36m2.8984\u001b[0m        \u001b[32m2.6600\u001b[0m  0.1426\n",
      "      4        \u001b[36m2.7422\u001b[0m        \u001b[32m2.5507\u001b[0m  0.1282\n",
      "      5        \u001b[36m2.6575\u001b[0m        \u001b[32m2.4902\u001b[0m  0.1454\n",
      "      6        \u001b[36m2.6092\u001b[0m        \u001b[32m2.4544\u001b[0m  0.1296\n",
      "      7        \u001b[36m2.5797\u001b[0m        \u001b[32m2.4318\u001b[0m  0.1426\n",
      "      8        \u001b[36m2.5605\u001b[0m        \u001b[32m2.4166\u001b[0m  0.1229\n",
      "      9        \u001b[36m2.5473\u001b[0m        \u001b[32m2.4058\u001b[0m  0.1199\n",
      "     10        \u001b[36m2.5378\u001b[0m        \u001b[32m2.3980\u001b[0m  0.1448\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4852\u001b[0m        \u001b[32m3.2605\u001b[0m  0.1246\n",
      "      2        \u001b[36m3.1555\u001b[0m        \u001b[32m2.9988\u001b[0m  0.1271\n",
      "      3        \u001b[36m2.9414\u001b[0m        \u001b[32m2.8330\u001b[0m  0.1126\n",
      "      4        \u001b[36m2.8049\u001b[0m        \u001b[32m2.7292\u001b[0m  0.1198\n",
      "      5        \u001b[36m2.7189\u001b[0m        \u001b[32m2.6639\u001b[0m  0.1304\n",
      "      6        \u001b[36m2.6645\u001b[0m        \u001b[32m2.6217\u001b[0m  0.1181\n",
      "      7        \u001b[36m2.6289\u001b[0m        \u001b[32m2.5936\u001b[0m  0.1351\n",
      "      8        \u001b[36m2.6048\u001b[0m        \u001b[32m2.5741\u001b[0m  0.1114\n",
      "      9        \u001b[36m2.5879\u001b[0m        \u001b[32m2.5601\u001b[0m  0.1348\n",
      "     10        \u001b[36m2.5755\u001b[0m        \u001b[32m2.5497\u001b[0m  0.1098\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3695\u001b[0m        \u001b[32m3.2021\u001b[0m  0.1326\n",
      "      2        \u001b[36m3.0599\u001b[0m        \u001b[32m2.9652\u001b[0m  0.1033\n",
      "      3        \u001b[36m2.8704\u001b[0m        \u001b[32m2.8165\u001b[0m  0.1111\n",
      "      4        \u001b[36m2.7540\u001b[0m        \u001b[32m2.7223\u001b[0m  0.1160\n",
      "      5        \u001b[36m2.6826\u001b[0m        \u001b[32m2.6622\u001b[0m  0.1287\n",
      "      6        \u001b[36m2.6363\u001b[0m        \u001b[32m2.6239\u001b[0m  0.1224\n",
      "      7        \u001b[36m2.6061\u001b[0m        \u001b[32m2.5993\u001b[0m  0.1157\n",
      "      8        \u001b[36m2.5858\u001b[0m        \u001b[32m2.5820\u001b[0m  0.1414\n",
      "      9        \u001b[36m2.5713\u001b[0m        \u001b[32m2.5688\u001b[0m  0.1219\n",
      "     10        \u001b[36m2.5603\u001b[0m        \u001b[32m2.5592\u001b[0m  0.1195\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5449\u001b[0m        \u001b[32m3.1635\u001b[0m  0.1175\n",
      "      2        \u001b[36m3.1495\u001b[0m        \u001b[32m2.8164\u001b[0m  0.1255\n",
      "      3        \u001b[36m2.9044\u001b[0m        \u001b[32m2.6772\u001b[0m  0.1306\n",
      "      4        \u001b[36m2.7658\u001b[0m        \u001b[32m2.5864\u001b[0m  0.1149\n",
      "      5        \u001b[36m2.6884\u001b[0m        \u001b[32m2.5288\u001b[0m  0.1262\n",
      "      6        \u001b[36m2.6423\u001b[0m        \u001b[32m2.4939\u001b[0m  0.1746\n",
      "      7        \u001b[36m2.6131\u001b[0m        \u001b[32m2.4711\u001b[0m  0.1201\n",
      "      8        \u001b[36m2.5935\u001b[0m        \u001b[32m2.4555\u001b[0m  0.1285\n",
      "      9        \u001b[36m2.5793\u001b[0m        \u001b[32m2.4503\u001b[0m  0.1074\n",
      "     10        \u001b[36m2.5689\u001b[0m        2.4720  0.1489\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5027\u001b[0m        \u001b[32m3.2809\u001b[0m  0.1560\n",
      "      2        \u001b[36m3.1270\u001b[0m        \u001b[32m3.0077\u001b[0m  0.1313\n",
      "      3        \u001b[36m2.8985\u001b[0m        \u001b[32m2.8313\u001b[0m  0.1154\n",
      "      4        \u001b[36m2.7659\u001b[0m        \u001b[32m2.7233\u001b[0m  0.1137\n",
      "      5        \u001b[36m2.6882\u001b[0m        \u001b[32m2.6384\u001b[0m  0.1097\n",
      "      6        \u001b[36m2.6412\u001b[0m        \u001b[32m2.6221\u001b[0m  0.1107\n",
      "      7        \u001b[36m2.6109\u001b[0m        \u001b[32m2.5927\u001b[0m  0.1148\n",
      "      8        \u001b[36m2.5907\u001b[0m        \u001b[32m2.5719\u001b[0m  0.1209\n",
      "      9        \u001b[36m2.5763\u001b[0m        \u001b[32m2.5648\u001b[0m  0.1172\n",
      "     10        \u001b[36m2.5660\u001b[0m        \u001b[32m2.5532\u001b[0m  0.1391\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4592\u001b[0m        \u001b[32m3.2102\u001b[0m  0.1330\n",
      "      2        \u001b[36m3.0803\u001b[0m        \u001b[32m2.9196\u001b[0m  0.1254\n",
      "      3        \u001b[36m2.8588\u001b[0m        \u001b[32m2.7983\u001b[0m  0.1336\n",
      "      4        \u001b[36m2.7236\u001b[0m        \u001b[32m2.6835\u001b[0m  0.1182\n",
      "      5        \u001b[36m2.6459\u001b[0m        \u001b[32m2.6335\u001b[0m  0.1219\n",
      "      6        \u001b[36m2.6003\u001b[0m        \u001b[32m2.5978\u001b[0m  0.1108\n",
      "      7        \u001b[36m2.5714\u001b[0m        \u001b[32m2.5738\u001b[0m  0.1130\n",
      "      8        \u001b[36m2.5525\u001b[0m        \u001b[32m2.5620\u001b[0m  0.1130\n",
      "      9        \u001b[36m2.5388\u001b[0m        \u001b[32m2.5501\u001b[0m  0.1244\n",
      "     10        \u001b[36m2.5290\u001b[0m        \u001b[32m2.5408\u001b[0m  0.1344\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5489\u001b[0m        \u001b[32m3.1460\u001b[0m  0.1093\n",
      "      2        \u001b[36m3.1378\u001b[0m        \u001b[32m2.8298\u001b[0m  0.1333\n",
      "      3        \u001b[36m2.8861\u001b[0m        \u001b[32m2.3576\u001b[0m  0.1239\n",
      "      4        \u001b[36m2.7434\u001b[0m        2.5477  0.1288\n",
      "      5        \u001b[36m2.6613\u001b[0m        2.4774  0.1435\n",
      "      6        \u001b[36m2.6140\u001b[0m        2.4600  0.1995\n",
      "      7        \u001b[36m2.5829\u001b[0m        2.4345  0.2329\n",
      "      8        \u001b[36m2.5627\u001b[0m        2.4226  0.1451\n",
      "      9        \u001b[36m2.5490\u001b[0m        2.4080  0.1621\n",
      "     10        \u001b[36m2.5385\u001b[0m        2.4244  0.1779\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5616\u001b[0m        \u001b[32m3.3027\u001b[0m  0.1378\n",
      "      2        \u001b[36m3.1492\u001b[0m        \u001b[32m2.8371\u001b[0m  0.1848\n",
      "      3        \u001b[36m2.9093\u001b[0m        \u001b[32m2.7815\u001b[0m  0.2235\n",
      "      4        \u001b[36m2.7738\u001b[0m        \u001b[32m2.6935\u001b[0m  0.1622\n",
      "      5        \u001b[36m2.7009\u001b[0m        \u001b[32m2.6504\u001b[0m  0.1850\n",
      "      6        \u001b[36m2.6590\u001b[0m        \u001b[32m2.6126\u001b[0m  0.1733\n",
      "      7        \u001b[36m2.6326\u001b[0m        \u001b[32m2.5930\u001b[0m  0.1477\n",
      "      8        \u001b[36m2.6146\u001b[0m        2.5954  0.1770\n",
      "      9        \u001b[36m2.5997\u001b[0m        \u001b[32m2.5746\u001b[0m  0.1750\n",
      "     10        \u001b[36m2.5899\u001b[0m        \u001b[32m2.5594\u001b[0m  0.1658\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4935\u001b[0m        \u001b[32m3.1582\u001b[0m  0.1461\n",
      "      2        \u001b[36m3.1107\u001b[0m        \u001b[32m3.0176\u001b[0m  0.1709\n",
      "      3        \u001b[36m2.8822\u001b[0m        \u001b[32m2.8358\u001b[0m  0.1470\n",
      "      4        \u001b[36m2.7508\u001b[0m        \u001b[32m2.6600\u001b[0m  0.1455\n",
      "      5        \u001b[36m2.6737\u001b[0m        \u001b[32m2.6415\u001b[0m  0.1619\n",
      "      6        \u001b[36m2.6278\u001b[0m        \u001b[32m2.6076\u001b[0m  0.1888\n",
      "      7        \u001b[36m2.5972\u001b[0m        \u001b[32m2.5743\u001b[0m  0.2190\n",
      "      8        \u001b[36m2.5769\u001b[0m        \u001b[32m2.5702\u001b[0m  0.1919\n",
      "      9        \u001b[36m2.5614\u001b[0m        \u001b[32m2.5692\u001b[0m  0.1506\n",
      "     10        \u001b[36m2.5520\u001b[0m        \u001b[32m2.5443\u001b[0m  0.1896\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3892\u001b[0m        \u001b[32m3.0206\u001b[0m  0.1603\n",
      "      2        \u001b[36m3.0700\u001b[0m        \u001b[32m2.8177\u001b[0m  0.1677\n",
      "      3        \u001b[36m2.8737\u001b[0m        \u001b[32m2.6706\u001b[0m  0.1635\n",
      "      4        \u001b[36m2.7511\u001b[0m        \u001b[32m2.5773\u001b[0m  0.1428\n",
      "      5        \u001b[36m2.6757\u001b[0m        \u001b[32m2.5224\u001b[0m  0.1325\n",
      "      6        \u001b[36m2.6288\u001b[0m        \u001b[32m2.4862\u001b[0m  0.1480\n",
      "      7        \u001b[36m2.5976\u001b[0m        \u001b[32m2.4613\u001b[0m  0.1780\n",
      "      8        \u001b[36m2.5760\u001b[0m        \u001b[32m2.4440\u001b[0m  0.1459\n",
      "      9        \u001b[36m2.5606\u001b[0m        \u001b[32m2.4311\u001b[0m  0.1684\n",
      "     10        \u001b[36m2.5492\u001b[0m        \u001b[32m2.4260\u001b[0m  0.1394\n",
      "     11        \u001b[36m2.5407\u001b[0m        \u001b[32m2.4102\u001b[0m  0.1709\n",
      "     12        \u001b[36m2.5334\u001b[0m        \u001b[32m2.4012\u001b[0m  0.1617\n",
      "     13        \u001b[36m2.5278\u001b[0m        \u001b[32m2.3948\u001b[0m  0.1563\n",
      "     14        \u001b[36m2.5233\u001b[0m        \u001b[32m2.3899\u001b[0m  0.1541\n",
      "     15        \u001b[36m2.5197\u001b[0m        \u001b[32m2.3862\u001b[0m  0.1252\n",
      "     16        \u001b[36m2.5167\u001b[0m        \u001b[32m2.3831\u001b[0m  0.1907\n",
      "     17        \u001b[36m2.5142\u001b[0m        \u001b[32m2.3805\u001b[0m  0.1236\n",
      "     18        \u001b[36m2.5121\u001b[0m        \u001b[32m2.3784\u001b[0m  0.1147\n",
      "     19        \u001b[36m2.5103\u001b[0m        \u001b[32m2.3766\u001b[0m  0.1871\n",
      "     20        \u001b[36m2.5087\u001b[0m        \u001b[32m2.3751\u001b[0m  0.1527\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4240\u001b[0m        \u001b[32m3.2678\u001b[0m  0.1583\n",
      "      2        \u001b[36m3.1059\u001b[0m        \u001b[32m2.9710\u001b[0m  0.1950\n",
      "      3        \u001b[36m2.9052\u001b[0m        \u001b[32m2.8008\u001b[0m  0.1592\n",
      "      4        \u001b[36m2.7815\u001b[0m        \u001b[32m2.7152\u001b[0m  0.1744\n",
      "      5        \u001b[36m2.7043\u001b[0m        \u001b[32m2.6561\u001b[0m  0.1586\n",
      "      6        \u001b[36m2.6548\u001b[0m        \u001b[32m2.6028\u001b[0m  0.1511\n",
      "      7        \u001b[36m2.6224\u001b[0m        \u001b[32m2.5920\u001b[0m  0.1512\n",
      "      8        \u001b[36m2.5998\u001b[0m        \u001b[32m2.5755\u001b[0m  0.1476\n",
      "      9        \u001b[36m2.5845\u001b[0m        \u001b[32m2.5698\u001b[0m  0.1684\n",
      "     10        \u001b[36m2.5728\u001b[0m        \u001b[32m2.5531\u001b[0m  0.1631\n",
      "     11        \u001b[36m2.5639\u001b[0m        \u001b[32m2.5474\u001b[0m  0.1811\n",
      "     12        \u001b[36m2.5571\u001b[0m        \u001b[32m2.5298\u001b[0m  0.1490\n",
      "     13        \u001b[36m2.5515\u001b[0m        2.5321  0.1584\n",
      "     14        \u001b[36m2.5472\u001b[0m        2.5313  0.1353\n",
      "     15        \u001b[36m2.5439\u001b[0m        \u001b[32m2.5190\u001b[0m  0.1357\n",
      "     16        \u001b[36m2.5409\u001b[0m        \u001b[32m2.5181\u001b[0m  0.1563\n",
      "     17        \u001b[36m2.5384\u001b[0m        \u001b[32m2.5168\u001b[0m  0.1642\n",
      "     18        \u001b[36m2.5364\u001b[0m        \u001b[32m2.5153\u001b[0m  0.1661\n",
      "     19        \u001b[36m2.5346\u001b[0m        \u001b[32m2.5137\u001b[0m  0.1684\n",
      "     20        \u001b[36m2.5330\u001b[0m        \u001b[32m2.5119\u001b[0m  0.1813\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3934\u001b[0m        \u001b[32m3.2225\u001b[0m  0.1351\n",
      "      2        \u001b[36m3.0621\u001b[0m        \u001b[32m2.9659\u001b[0m  0.1860\n",
      "      3        \u001b[36m2.8612\u001b[0m        \u001b[32m2.8091\u001b[0m  0.1439\n",
      "      4        \u001b[36m2.7385\u001b[0m        \u001b[32m2.7134\u001b[0m  0.1268\n",
      "      5        \u001b[36m2.6630\u001b[0m        \u001b[32m2.6537\u001b[0m  0.1367\n",
      "      6        \u001b[36m2.6153\u001b[0m        \u001b[32m2.6152\u001b[0m  0.2055\n",
      "      7        \u001b[36m2.5839\u001b[0m        \u001b[32m2.5894\u001b[0m  0.1427\n",
      "      8        \u001b[36m2.5624\u001b[0m        \u001b[32m2.5713\u001b[0m  0.1686\n",
      "      9        \u001b[36m2.5471\u001b[0m        \u001b[32m2.5582\u001b[0m  0.1780\n",
      "     10        \u001b[36m2.5359\u001b[0m        \u001b[32m2.5484\u001b[0m  0.1406\n",
      "     11        \u001b[36m2.5274\u001b[0m        \u001b[32m2.5409\u001b[0m  0.1476\n",
      "     12        \u001b[36m2.5209\u001b[0m        \u001b[32m2.5351\u001b[0m  0.1221\n",
      "     13        \u001b[36m2.5157\u001b[0m        \u001b[32m2.5304\u001b[0m  0.1522\n",
      "     14        \u001b[36m2.5115\u001b[0m        \u001b[32m2.5266\u001b[0m  0.1775\n",
      "     15        \u001b[36m2.5081\u001b[0m        \u001b[32m2.5234\u001b[0m  0.1456\n",
      "     16        \u001b[36m2.5052\u001b[0m        \u001b[32m2.5208\u001b[0m  0.1502\n",
      "     17        \u001b[36m2.5028\u001b[0m        \u001b[32m2.5185\u001b[0m  0.1561\n",
      "     18        \u001b[36m2.5008\u001b[0m        \u001b[32m2.5166\u001b[0m  0.1328\n",
      "     19        \u001b[36m2.4991\u001b[0m        \u001b[32m2.5150\u001b[0m  0.1332\n",
      "     20        \u001b[36m2.4976\u001b[0m        \u001b[32m2.5136\u001b[0m  0.1096\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6430\u001b[0m        \u001b[32m3.2320\u001b[0m  0.1376\n",
      "      2        \u001b[36m3.2027\u001b[0m        \u001b[32m2.8962\u001b[0m  0.1616\n",
      "      3        \u001b[36m2.9287\u001b[0m        \u001b[32m2.6943\u001b[0m  0.1399\n",
      "      4        \u001b[36m2.7805\u001b[0m        \u001b[32m2.5871\u001b[0m  0.1571\n",
      "      5        \u001b[36m2.7022\u001b[0m        \u001b[32m2.5331\u001b[0m  0.1439\n",
      "      6        \u001b[36m2.6555\u001b[0m        \u001b[32m2.4975\u001b[0m  0.1494\n",
      "      7        \u001b[36m2.6244\u001b[0m        \u001b[32m2.4784\u001b[0m  0.1464\n",
      "      8        \u001b[36m2.6059\u001b[0m        \u001b[32m2.4640\u001b[0m  0.1337\n",
      "      9        \u001b[36m2.5917\u001b[0m        \u001b[32m2.4525\u001b[0m  0.1689\n",
      "     10        \u001b[36m2.5767\u001b[0m        \u001b[32m2.4433\u001b[0m  0.1287\n",
      "     11        \u001b[36m2.5678\u001b[0m        \u001b[32m2.4325\u001b[0m  0.1311\n",
      "     12        \u001b[36m2.5609\u001b[0m        \u001b[32m2.4303\u001b[0m  0.1253\n",
      "     13        \u001b[36m2.5563\u001b[0m        \u001b[32m2.4270\u001b[0m  0.1576\n",
      "     14        \u001b[36m2.5524\u001b[0m        \u001b[32m2.4213\u001b[0m  0.1687\n",
      "     15        \u001b[36m2.5496\u001b[0m        \u001b[32m2.4189\u001b[0m  0.1310\n",
      "     16        \u001b[36m2.5470\u001b[0m        \u001b[32m2.4160\u001b[0m  0.1351\n",
      "     17        \u001b[36m2.5447\u001b[0m        \u001b[32m2.4144\u001b[0m  0.1502\n",
      "     18        \u001b[36m2.5429\u001b[0m        \u001b[32m2.4123\u001b[0m  0.1278\n",
      "     19        \u001b[36m2.5414\u001b[0m        \u001b[32m2.4109\u001b[0m  0.1594\n",
      "     20        \u001b[36m2.5401\u001b[0m        \u001b[32m2.4087\u001b[0m  0.1296\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5798\u001b[0m        \u001b[32m3.3451\u001b[0m  0.1217\n",
      "      2        \u001b[36m3.1791\u001b[0m        \u001b[32m2.9095\u001b[0m  0.1348\n",
      "      3        \u001b[36m2.9257\u001b[0m        \u001b[32m2.8968\u001b[0m  0.1422\n",
      "      4        \u001b[36m2.7782\u001b[0m        \u001b[32m2.6456\u001b[0m  0.1490\n",
      "      5        \u001b[36m2.6935\u001b[0m        \u001b[32m2.6126\u001b[0m  0.1530\n",
      "      6        \u001b[36m2.6427\u001b[0m        2.6286  0.1768\n",
      "      7        \u001b[36m2.6113\u001b[0m        \u001b[32m2.5715\u001b[0m  0.1663\n",
      "      8        \u001b[36m2.5907\u001b[0m        \u001b[32m2.5621\u001b[0m  0.2005\n",
      "      9        \u001b[36m2.5761\u001b[0m        \u001b[32m2.5535\u001b[0m  0.1438\n",
      "     10        \u001b[36m2.5657\u001b[0m        \u001b[32m2.5436\u001b[0m  0.1120\n",
      "     11        \u001b[36m2.5576\u001b[0m        \u001b[32m2.5326\u001b[0m  0.1369\n",
      "     12        \u001b[36m2.5516\u001b[0m        \u001b[32m2.5298\u001b[0m  0.1384\n",
      "     13        \u001b[36m2.5472\u001b[0m        \u001b[32m2.5230\u001b[0m  0.1814\n",
      "     14        \u001b[36m2.5435\u001b[0m        \u001b[32m2.5224\u001b[0m  0.1674\n",
      "     15        \u001b[36m2.5402\u001b[0m        \u001b[32m2.5164\u001b[0m  0.1227\n",
      "     16        \u001b[36m2.5377\u001b[0m        \u001b[32m2.5161\u001b[0m  0.1420\n",
      "     17        \u001b[36m2.5355\u001b[0m        2.5165  0.1512\n",
      "     18        \u001b[36m2.5337\u001b[0m        \u001b[32m2.5125\u001b[0m  0.1286\n",
      "     19        \u001b[36m2.5322\u001b[0m        2.5127  0.1356\n",
      "     20        \u001b[36m2.5308\u001b[0m        \u001b[32m2.5098\u001b[0m  0.1129\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6130\u001b[0m        \u001b[32m3.3441\u001b[0m  0.1581\n",
      "      2        \u001b[36m3.1770\u001b[0m        \u001b[32m3.0361\u001b[0m  0.1634\n",
      "      3        \u001b[36m2.9023\u001b[0m        \u001b[32m2.8268\u001b[0m  0.1193\n",
      "      4        \u001b[36m2.7448\u001b[0m        \u001b[32m2.7069\u001b[0m  0.1364\n",
      "      5        \u001b[36m2.6568\u001b[0m        \u001b[32m2.6457\u001b[0m  0.1874\n",
      "      6        \u001b[36m2.6055\u001b[0m        \u001b[32m2.6068\u001b[0m  0.1416\n",
      "      7        \u001b[36m2.5739\u001b[0m        \u001b[32m2.5823\u001b[0m  0.1280\n",
      "      8        \u001b[36m2.5531\u001b[0m        \u001b[32m2.5658\u001b[0m  0.1629\n",
      "      9        \u001b[36m2.5387\u001b[0m        \u001b[32m2.5547\u001b[0m  0.1651\n",
      "     10        \u001b[36m2.5282\u001b[0m        \u001b[32m2.5429\u001b[0m  0.1179\n",
      "     11        \u001b[36m2.5209\u001b[0m        \u001b[32m2.5349\u001b[0m  0.1199\n",
      "     12        \u001b[36m2.5154\u001b[0m        \u001b[32m2.5328\u001b[0m  0.1567\n",
      "     13        \u001b[36m2.5107\u001b[0m        \u001b[32m2.5260\u001b[0m  0.1641\n",
      "     14        \u001b[36m2.5071\u001b[0m        \u001b[32m2.5235\u001b[0m  0.1821\n",
      "     15        \u001b[36m2.5041\u001b[0m        \u001b[32m2.5200\u001b[0m  0.1428\n",
      "     16        \u001b[36m2.5016\u001b[0m        \u001b[32m2.5180\u001b[0m  0.1338\n",
      "     17        \u001b[36m2.4996\u001b[0m        \u001b[32m2.5163\u001b[0m  0.1249\n",
      "     18        \u001b[36m2.4978\u001b[0m        \u001b[32m2.5148\u001b[0m  0.1448\n",
      "     19        \u001b[36m2.4963\u001b[0m        \u001b[32m2.5136\u001b[0m  0.1396\n",
      "     20        \u001b[36m2.4951\u001b[0m        \u001b[32m2.5125\u001b[0m  0.1569\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6067\u001b[0m        \u001b[32m3.2007\u001b[0m  0.1398\n",
      "      2        \u001b[36m3.1648\u001b[0m        \u001b[32m2.8734\u001b[0m  0.1492\n",
      "      3        \u001b[36m2.8970\u001b[0m        \u001b[32m2.6092\u001b[0m  0.1750\n",
      "      4        \u001b[36m2.7491\u001b[0m        \u001b[32m2.5597\u001b[0m  0.1846\n",
      "      5        \u001b[36m2.6662\u001b[0m        \u001b[32m2.5034\u001b[0m  0.1717\n",
      "      6        \u001b[36m2.6167\u001b[0m        \u001b[32m2.4662\u001b[0m  0.1627\n",
      "      7        \u001b[36m2.5840\u001b[0m        2.4715  0.1719\n",
      "      8        \u001b[36m2.5628\u001b[0m        \u001b[32m2.4251\u001b[0m  0.1439\n",
      "      9        \u001b[36m2.5488\u001b[0m        \u001b[32m2.4154\u001b[0m  0.1600\n",
      "     10        \u001b[36m2.5385\u001b[0m        \u001b[32m2.4030\u001b[0m  0.1781\n",
      "     11        \u001b[36m2.5309\u001b[0m        \u001b[32m2.3955\u001b[0m  0.1385\n",
      "     12        \u001b[36m2.5252\u001b[0m        \u001b[32m2.3901\u001b[0m  0.1502\n",
      "     13        \u001b[36m2.5207\u001b[0m        \u001b[32m2.3857\u001b[0m  0.1217\n",
      "     14        \u001b[36m2.5176\u001b[0m        2.3894  0.1629\n",
      "     15        \u001b[36m2.5150\u001b[0m        2.3861  0.1533\n",
      "     16        \u001b[36m2.5124\u001b[0m        \u001b[32m2.3830\u001b[0m  0.1519\n",
      "     17        \u001b[36m2.5102\u001b[0m        \u001b[32m2.3804\u001b[0m  0.1759\n",
      "     18        \u001b[36m2.5084\u001b[0m        \u001b[32m2.3753\u001b[0m  0.1436\n",
      "     19        \u001b[36m2.5068\u001b[0m        2.3765  0.1418\n",
      "     20        \u001b[36m2.5055\u001b[0m        \u001b[32m2.3745\u001b[0m  0.1274\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6289\u001b[0m        \u001b[32m3.3809\u001b[0m  0.1341\n",
      "      2        \u001b[36m3.1910\u001b[0m        \u001b[32m2.9539\u001b[0m  0.1440\n",
      "      3        \u001b[36m2.9213\u001b[0m        \u001b[32m2.7837\u001b[0m  0.1301\n",
      "      4        \u001b[36m2.7680\u001b[0m        \u001b[32m2.7006\u001b[0m  0.1321\n",
      "      5        \u001b[36m2.6827\u001b[0m        \u001b[32m2.6175\u001b[0m  0.1386\n",
      "      6        \u001b[36m2.6342\u001b[0m        \u001b[32m2.6015\u001b[0m  0.1482\n",
      "      7        \u001b[36m2.6042\u001b[0m        \u001b[32m2.5761\u001b[0m  0.1387\n",
      "      8        \u001b[36m2.5849\u001b[0m        \u001b[32m2.5608\u001b[0m  0.1452\n",
      "      9        \u001b[36m2.5716\u001b[0m        \u001b[32m2.5501\u001b[0m  0.1576\n",
      "     10        \u001b[36m2.5618\u001b[0m        \u001b[32m2.5401\u001b[0m  0.1280\n",
      "     11        \u001b[36m2.5546\u001b[0m        \u001b[32m2.5390\u001b[0m  0.1651\n",
      "     12        \u001b[36m2.5498\u001b[0m        \u001b[32m2.5321\u001b[0m  0.1506\n",
      "     13        \u001b[36m2.5452\u001b[0m        \u001b[32m2.5247\u001b[0m  0.1248\n",
      "     14        \u001b[36m2.5413\u001b[0m        \u001b[32m2.5230\u001b[0m  0.1162\n",
      "     15        \u001b[36m2.5385\u001b[0m        \u001b[32m2.5200\u001b[0m  0.1257\n",
      "     16        \u001b[36m2.5361\u001b[0m        \u001b[32m2.5155\u001b[0m  0.1408\n",
      "     17        \u001b[36m2.5342\u001b[0m        \u001b[32m2.5139\u001b[0m  0.1375\n",
      "     18        \u001b[36m2.5325\u001b[0m        \u001b[32m2.5117\u001b[0m  0.1278\n",
      "     19        \u001b[36m2.5311\u001b[0m        \u001b[32m2.5105\u001b[0m  0.1277\n",
      "     20        \u001b[36m2.5298\u001b[0m        \u001b[32m2.5101\u001b[0m  0.1430\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5685\u001b[0m        \u001b[32m3.2609\u001b[0m  0.1530\n",
      "      2        \u001b[36m3.1476\u001b[0m        \u001b[32m3.0065\u001b[0m  0.1189\n",
      "      3        \u001b[36m2.8882\u001b[0m        \u001b[32m2.7987\u001b[0m  0.1423\n",
      "      4        \u001b[36m2.7467\u001b[0m        \u001b[32m2.7150\u001b[0m  0.1590\n",
      "      5        \u001b[36m2.6708\u001b[0m        \u001b[32m2.6442\u001b[0m  0.1283\n",
      "      6        \u001b[36m2.6251\u001b[0m        \u001b[32m2.6127\u001b[0m  0.1306\n",
      "      7        \u001b[36m2.5956\u001b[0m        \u001b[32m2.5735\u001b[0m  0.1521\n",
      "      8        \u001b[36m2.5787\u001b[0m        2.5774  0.1701\n",
      "      9        \u001b[36m2.5647\u001b[0m        \u001b[32m2.5511\u001b[0m  0.1387\n",
      "     10        \u001b[36m2.5540\u001b[0m        2.5581  0.1447\n",
      "     11        \u001b[36m2.5478\u001b[0m        \u001b[32m2.5378\u001b[0m  0.1343\n",
      "     12        \u001b[36m2.5417\u001b[0m        \u001b[32m2.5331\u001b[0m  0.1892\n",
      "     13        \u001b[36m2.5369\u001b[0m        2.5358  0.1554\n",
      "     14        \u001b[36m2.5310\u001b[0m        \u001b[32m2.5299\u001b[0m  0.1595\n",
      "     15        \u001b[36m2.5262\u001b[0m        \u001b[32m2.5275\u001b[0m  0.1485\n",
      "     16        \u001b[36m2.5231\u001b[0m        \u001b[32m2.5213\u001b[0m  0.1603\n",
      "     17        \u001b[36m2.5203\u001b[0m        2.5241  0.1725\n",
      "     18        \u001b[36m2.5177\u001b[0m        2.5226  0.1838\n",
      "     19        \u001b[36m2.5156\u001b[0m        2.5217  0.1649\n",
      "     20        \u001b[36m2.5139\u001b[0m        \u001b[32m2.5197\u001b[0m  0.1540\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3720\u001b[0m        \u001b[32m2.8214\u001b[0m  0.1589\n",
      "      2        \u001b[36m2.8016\u001b[0m        \u001b[32m2.5373\u001b[0m  0.1627\n",
      "      3        \u001b[36m2.6254\u001b[0m        \u001b[32m2.4490\u001b[0m  0.1578\n",
      "      4        \u001b[36m2.5658\u001b[0m        \u001b[32m2.4138\u001b[0m  0.1781\n",
      "      5        \u001b[36m2.5396\u001b[0m        \u001b[32m2.3963\u001b[0m  0.1624\n",
      "      6        \u001b[36m2.5258\u001b[0m        \u001b[32m2.3863\u001b[0m  0.1439\n",
      "      7        \u001b[36m2.5174\u001b[0m        \u001b[32m2.3800\u001b[0m  0.2210\n",
      "      8        \u001b[36m2.5120\u001b[0m        \u001b[32m2.3757\u001b[0m  0.1756\n",
      "      9        \u001b[36m2.5082\u001b[0m        \u001b[32m2.3726\u001b[0m  0.1549\n",
      "     10        \u001b[36m2.5054\u001b[0m        \u001b[32m2.3704\u001b[0m  0.1449\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2556\u001b[0m        \u001b[32m2.9507\u001b[0m  0.1632\n",
      "      2        \u001b[36m2.8338\u001b[0m        \u001b[32m2.7046\u001b[0m  0.1420\n",
      "      3        \u001b[36m2.6789\u001b[0m        \u001b[32m2.6119\u001b[0m  0.1465\n",
      "      4        \u001b[36m2.6132\u001b[0m        \u001b[32m2.5703\u001b[0m  0.1270\n",
      "      5        \u001b[36m2.5821\u001b[0m        \u001b[32m2.5488\u001b[0m  0.1904\n",
      "      6        \u001b[36m2.5651\u001b[0m        \u001b[32m2.5363\u001b[0m  0.1279\n",
      "      7        \u001b[36m2.5568\u001b[0m        \u001b[32m2.5244\u001b[0m  0.1674\n",
      "      8        \u001b[36m2.5534\u001b[0m        \u001b[32m2.5226\u001b[0m  0.1802\n",
      "      9        \u001b[36m2.5395\u001b[0m        \u001b[32m2.5176\u001b[0m  0.1659\n",
      "     10        \u001b[36m2.5352\u001b[0m        \u001b[32m2.5140\u001b[0m  0.1522\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4042\u001b[0m        \u001b[32m3.0417\u001b[0m  0.1721\n",
      "      2        \u001b[36m2.8297\u001b[0m        \u001b[32m2.7145\u001b[0m  0.1538\n",
      "      3        \u001b[36m2.6445\u001b[0m        \u001b[32m2.6138\u001b[0m  0.1393\n",
      "      4        \u001b[36m2.5833\u001b[0m        \u001b[32m2.5727\u001b[0m  0.1643\n",
      "      5        \u001b[36m2.5537\u001b[0m        \u001b[32m2.5516\u001b[0m  0.1939\n",
      "      6        \u001b[36m2.5273\u001b[0m        \u001b[32m2.5345\u001b[0m  0.1329\n",
      "      7        \u001b[36m2.5129\u001b[0m        \u001b[32m2.5249\u001b[0m  0.1064\n",
      "      8        \u001b[36m2.5052\u001b[0m        \u001b[32m2.5188\u001b[0m  0.1366\n",
      "      9        \u001b[36m2.5003\u001b[0m        \u001b[32m2.5147\u001b[0m  0.1691\n",
      "     10        \u001b[36m2.4968\u001b[0m        \u001b[32m2.5118\u001b[0m  0.1378\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3128\u001b[0m        \u001b[32m2.8371\u001b[0m  0.1752\n",
      "      2        \u001b[36m2.8322\u001b[0m        \u001b[32m2.5832\u001b[0m  0.1373\n",
      "      3        \u001b[36m2.6543\u001b[0m        \u001b[32m2.4493\u001b[0m  0.1231\n",
      "      4        \u001b[36m2.5837\u001b[0m        \u001b[32m2.4317\u001b[0m  0.1200\n",
      "      5        \u001b[36m2.5515\u001b[0m        \u001b[32m2.4107\u001b[0m  0.1695\n",
      "      6        \u001b[36m2.5342\u001b[0m        \u001b[32m2.3962\u001b[0m  0.1662\n",
      "      7        \u001b[36m2.5236\u001b[0m        \u001b[32m2.3906\u001b[0m  0.1286\n",
      "      8        \u001b[36m2.5167\u001b[0m        \u001b[32m2.3843\u001b[0m  0.1540\n",
      "      9        \u001b[36m2.5118\u001b[0m        \u001b[32m2.3752\u001b[0m  0.1652\n",
      "     10        \u001b[36m2.5084\u001b[0m        \u001b[32m2.3737\u001b[0m  0.1559\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3783\u001b[0m        \u001b[32m2.9893\u001b[0m  0.1372\n",
      "      2        \u001b[36m2.8288\u001b[0m        \u001b[32m2.6911\u001b[0m  0.1422\n",
      "      3        \u001b[36m2.6541\u001b[0m        \u001b[32m2.5978\u001b[0m  0.1491\n",
      "      4        \u001b[36m2.5929\u001b[0m        \u001b[32m2.5591\u001b[0m  0.1542\n",
      "      5        \u001b[36m2.5658\u001b[0m        \u001b[32m2.5398\u001b[0m  0.1539\n",
      "      6        \u001b[36m2.5514\u001b[0m        \u001b[32m2.5284\u001b[0m  0.1171\n",
      "      7        \u001b[36m2.5427\u001b[0m        \u001b[32m2.5217\u001b[0m  0.1563\n",
      "      8        \u001b[36m2.5372\u001b[0m        \u001b[32m2.5168\u001b[0m  0.1263\n",
      "      9        \u001b[36m2.5334\u001b[0m        \u001b[32m2.5136\u001b[0m  0.1650\n",
      "     10        \u001b[36m2.5305\u001b[0m        \u001b[32m2.5110\u001b[0m  0.1289\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4248\u001b[0m        \u001b[32m3.0510\u001b[0m  0.1561\n",
      "      2        \u001b[36m2.8286\u001b[0m        \u001b[32m2.7059\u001b[0m  0.1338\n",
      "      3        \u001b[36m2.6295\u001b[0m        \u001b[32m2.6013\u001b[0m  0.1525\n",
      "      4        \u001b[36m2.5621\u001b[0m        \u001b[32m2.5603\u001b[0m  0.1377\n",
      "      5        \u001b[36m2.5327\u001b[0m        \u001b[32m2.5403\u001b[0m  0.1353\n",
      "      6        \u001b[36m2.5176\u001b[0m        \u001b[32m2.5288\u001b[0m  0.1575\n",
      "      7        \u001b[36m2.5085\u001b[0m        \u001b[32m2.5217\u001b[0m  0.1471\n",
      "      8        \u001b[36m2.5026\u001b[0m        \u001b[32m2.5169\u001b[0m  0.1366\n",
      "      9        \u001b[36m2.4985\u001b[0m        \u001b[32m2.5134\u001b[0m  0.1253\n",
      "     10        \u001b[36m2.4956\u001b[0m        \u001b[32m2.5109\u001b[0m  0.1308\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3899\u001b[0m        \u001b[32m2.9267\u001b[0m  0.1370\n",
      "      2        \u001b[36m2.8333\u001b[0m        \u001b[32m2.5490\u001b[0m  0.1694\n",
      "      3        \u001b[36m2.6417\u001b[0m        \u001b[32m2.4596\u001b[0m  0.1481\n",
      "      4        \u001b[36m2.5749\u001b[0m        \u001b[32m2.4098\u001b[0m  0.1525\n",
      "      5        \u001b[36m2.5453\u001b[0m        \u001b[32m2.3904\u001b[0m  0.1272\n",
      "      6        \u001b[36m2.5293\u001b[0m        \u001b[32m2.3898\u001b[0m  0.1282\n",
      "      7        \u001b[36m2.5197\u001b[0m        \u001b[32m2.3878\u001b[0m  0.1519\n",
      "      8        \u001b[36m2.5146\u001b[0m        \u001b[32m2.3775\u001b[0m  0.1675\n",
      "      9        \u001b[36m2.5098\u001b[0m        \u001b[32m2.3739\u001b[0m  0.1644\n",
      "     10        \u001b[36m2.5067\u001b[0m        \u001b[32m2.3713\u001b[0m  0.1935\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3537\u001b[0m        \u001b[32m3.0201\u001b[0m  0.1274\n",
      "      2        \u001b[36m2.8377\u001b[0m        \u001b[32m2.7124\u001b[0m  0.2507\n",
      "      3        \u001b[36m2.6634\u001b[0m        \u001b[32m2.6122\u001b[0m  0.1247\n",
      "      4        \u001b[36m2.5991\u001b[0m        \u001b[32m2.5664\u001b[0m  0.1085\n",
      "      5        \u001b[36m2.5703\u001b[0m        \u001b[32m2.5448\u001b[0m  0.2096\n",
      "      6        \u001b[36m2.5546\u001b[0m        \u001b[32m2.5326\u001b[0m  0.1675\n",
      "      7        \u001b[36m2.5455\u001b[0m        \u001b[32m2.5251\u001b[0m  0.1311\n",
      "      8        \u001b[36m2.5393\u001b[0m        \u001b[32m2.5202\u001b[0m  0.1676\n",
      "      9        \u001b[36m2.5349\u001b[0m        \u001b[32m2.5165\u001b[0m  0.1743\n",
      "     10        \u001b[36m2.5317\u001b[0m        \u001b[32m2.5140\u001b[0m  0.1618\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3945\u001b[0m        \u001b[32m3.0418\u001b[0m  0.1266\n",
      "      2        \u001b[36m2.8448\u001b[0m        \u001b[32m2.7247\u001b[0m  0.1365\n",
      "      3        \u001b[36m2.6566\u001b[0m        \u001b[32m2.6250\u001b[0m  0.1752\n",
      "      4        \u001b[36m2.5929\u001b[0m        \u001b[32m2.5804\u001b[0m  0.1266\n",
      "      5        \u001b[36m2.5627\u001b[0m        \u001b[32m2.5584\u001b[0m  0.1400\n",
      "      6        \u001b[36m2.5466\u001b[0m        \u001b[32m2.5418\u001b[0m  0.1571\n",
      "      7        \u001b[36m2.5367\u001b[0m        \u001b[32m2.5337\u001b[0m  0.1066\n",
      "      8        \u001b[36m2.5303\u001b[0m        \u001b[32m2.5289\u001b[0m  0.1254\n",
      "      9        \u001b[36m2.5267\u001b[0m        \u001b[32m2.5249\u001b[0m  0.1577\n",
      "     10        \u001b[36m2.5233\u001b[0m        \u001b[32m2.5219\u001b[0m  0.1466\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5316\u001b[0m        \u001b[32m2.9701\u001b[0m  0.1046\n",
      "      2        \u001b[36m2.9295\u001b[0m        \u001b[32m2.6122\u001b[0m  0.1181\n",
      "      3        \u001b[36m2.6882\u001b[0m        \u001b[32m2.4827\u001b[0m  0.1362\n",
      "      4        \u001b[36m2.5946\u001b[0m        \u001b[32m2.4305\u001b[0m  0.1183\n",
      "      5        \u001b[36m2.5547\u001b[0m        \u001b[32m2.4058\u001b[0m  0.1056\n",
      "      6        \u001b[36m2.5347\u001b[0m        \u001b[32m2.3922\u001b[0m  0.1096\n",
      "      7        \u001b[36m2.5233\u001b[0m        \u001b[32m2.3840\u001b[0m  0.1412\n",
      "      8        \u001b[36m2.5161\u001b[0m        \u001b[32m2.3786\u001b[0m  0.1350\n",
      "      9        \u001b[36m2.5113\u001b[0m        \u001b[32m2.3748\u001b[0m  0.1243\n",
      "     10        \u001b[36m2.5078\u001b[0m        \u001b[32m2.3720\u001b[0m  0.1127\n",
      "     11        \u001b[36m2.5052\u001b[0m        \u001b[32m2.3699\u001b[0m  0.1538\n",
      "     12        \u001b[36m2.5032\u001b[0m        \u001b[32m2.3683\u001b[0m  0.1569\n",
      "     13        \u001b[36m2.5017\u001b[0m        \u001b[32m2.3670\u001b[0m  0.1284\n",
      "     14        \u001b[36m2.5004\u001b[0m        \u001b[32m2.3659\u001b[0m  0.1059\n",
      "     15        \u001b[36m2.4994\u001b[0m        \u001b[32m2.3650\u001b[0m  0.1539\n",
      "     16        \u001b[36m2.4985\u001b[0m        \u001b[32m2.3643\u001b[0m  0.1157\n",
      "     17        \u001b[36m2.4978\u001b[0m        \u001b[32m2.3637\u001b[0m  0.1139\n",
      "     18        \u001b[36m2.4972\u001b[0m        \u001b[32m2.3631\u001b[0m  0.1153\n",
      "     19        \u001b[36m2.4967\u001b[0m        \u001b[32m2.3627\u001b[0m  0.0987\n",
      "     20        \u001b[36m2.4962\u001b[0m        \u001b[32m2.3623\u001b[0m  0.1424\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4509\u001b[0m        \u001b[32m3.1318\u001b[0m  0.1333\n",
      "      2        \u001b[36m2.9721\u001b[0m        \u001b[32m2.8251\u001b[0m  0.1320\n",
      "      3        \u001b[36m2.7621\u001b[0m        \u001b[32m2.6848\u001b[0m  0.1244\n",
      "      4        \u001b[36m2.6626\u001b[0m        \u001b[32m2.6150\u001b[0m  0.1303\n",
      "      5        \u001b[36m2.6115\u001b[0m        \u001b[32m2.5772\u001b[0m  0.1130\n",
      "      6        \u001b[36m2.5829\u001b[0m        \u001b[32m2.5550\u001b[0m  0.1419\n",
      "      7        \u001b[36m2.5655\u001b[0m        \u001b[32m2.5411\u001b[0m  0.1252\n",
      "      8        \u001b[36m2.5542\u001b[0m        \u001b[32m2.5317\u001b[0m  0.1704\n",
      "      9        \u001b[36m2.5465\u001b[0m        \u001b[32m2.5252\u001b[0m  0.1489\n",
      "     10        \u001b[36m2.5410\u001b[0m        \u001b[32m2.5205\u001b[0m  0.1237\n",
      "     11        \u001b[36m2.5369\u001b[0m        \u001b[32m2.5169\u001b[0m  0.1338\n",
      "     12        \u001b[36m2.5338\u001b[0m        \u001b[32m2.5141\u001b[0m  0.1067\n",
      "     13        \u001b[36m2.5314\u001b[0m        \u001b[32m2.5119\u001b[0m  0.1467\n",
      "     14        \u001b[36m2.5294\u001b[0m        \u001b[32m2.5101\u001b[0m  0.1368\n",
      "     15        \u001b[36m2.5279\u001b[0m        \u001b[32m2.5087\u001b[0m  0.1339\n",
      "     16        \u001b[36m2.5265\u001b[0m        \u001b[32m2.5075\u001b[0m  0.1088\n",
      "     17        \u001b[36m2.5254\u001b[0m        \u001b[32m2.5064\u001b[0m  0.1124\n",
      "     18        \u001b[36m2.5245\u001b[0m        \u001b[32m2.5056\u001b[0m  0.1265\n",
      "     19        \u001b[36m2.5237\u001b[0m        \u001b[32m2.5048\u001b[0m  0.1115\n",
      "     20        \u001b[36m2.5230\u001b[0m        \u001b[32m2.5041\u001b[0m  0.1501\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3615\u001b[0m        \u001b[32m3.0409\u001b[0m  0.1026\n",
      "      2        \u001b[36m2.8417\u001b[0m        \u001b[32m2.7093\u001b[0m  0.1151\n",
      "      3        \u001b[36m2.6590\u001b[0m        \u001b[32m2.6104\u001b[0m  0.1209\n",
      "      4        \u001b[36m2.5916\u001b[0m        \u001b[32m2.5785\u001b[0m  0.1463\n",
      "      5        \u001b[36m2.5614\u001b[0m        \u001b[32m2.5480\u001b[0m  0.1054\n",
      "      6        \u001b[36m2.5456\u001b[0m        \u001b[32m2.5391\u001b[0m  0.1252\n",
      "      7        \u001b[36m2.5363\u001b[0m        \u001b[32m2.5323\u001b[0m  0.1054\n",
      "      8        \u001b[36m2.5303\u001b[0m        \u001b[32m2.5277\u001b[0m  0.1849\n",
      "      9        \u001b[36m2.5260\u001b[0m        \u001b[32m2.5244\u001b[0m  0.1105\n",
      "     10        \u001b[36m2.5228\u001b[0m        \u001b[32m2.5215\u001b[0m  0.1515\n",
      "     11        \u001b[36m2.5174\u001b[0m        \u001b[32m2.5182\u001b[0m  0.1219\n",
      "     12        \u001b[36m2.5135\u001b[0m        \u001b[32m2.5159\u001b[0m  0.1196\n",
      "     13        \u001b[36m2.5116\u001b[0m        \u001b[32m2.5140\u001b[0m  0.1276\n",
      "     14        \u001b[36m2.5084\u001b[0m        \u001b[32m2.5124\u001b[0m  0.1217\n",
      "     15        \u001b[36m2.5048\u001b[0m        \u001b[32m2.5110\u001b[0m  0.1102\n",
      "     16        \u001b[36m2.4915\u001b[0m        \u001b[32m2.5042\u001b[0m  0.1423\n",
      "     17        \u001b[36m2.4879\u001b[0m        \u001b[32m2.5035\u001b[0m  0.1075\n",
      "     18        \u001b[36m2.4872\u001b[0m        \u001b[32m2.5029\u001b[0m  0.1524\n",
      "     19        \u001b[36m2.4866\u001b[0m        \u001b[32m2.5024\u001b[0m  0.1620\n",
      "     20        \u001b[36m2.4860\u001b[0m        \u001b[32m2.5020\u001b[0m  0.1294\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2832\u001b[0m        \u001b[32m2.8226\u001b[0m  0.1039\n",
      "      2        \u001b[36m2.8097\u001b[0m        \u001b[32m2.5633\u001b[0m  0.1721\n",
      "      3        \u001b[36m2.6501\u001b[0m        \u001b[32m2.4751\u001b[0m  0.1578\n",
      "      4        \u001b[36m2.5894\u001b[0m        \u001b[32m2.4339\u001b[0m  0.1083\n",
      "      5        \u001b[36m2.5583\u001b[0m        \u001b[32m2.4149\u001b[0m  0.1208\n",
      "      6        \u001b[36m2.5389\u001b[0m        \u001b[32m2.4019\u001b[0m  0.1087\n",
      "      7        \u001b[36m2.5269\u001b[0m        \u001b[32m2.3941\u001b[0m  0.1674\n",
      "      8        \u001b[36m2.5191\u001b[0m        \u001b[32m2.3886\u001b[0m  0.1258\n",
      "      9        \u001b[36m2.5139\u001b[0m        \u001b[32m2.3852\u001b[0m  0.1171\n",
      "     10        \u001b[36m2.5105\u001b[0m        \u001b[32m2.3827\u001b[0m  0.1845\n",
      "     11        \u001b[36m2.5079\u001b[0m        \u001b[32m2.3798\u001b[0m  0.2286\n",
      "     12        \u001b[36m2.5044\u001b[0m        \u001b[32m2.3781\u001b[0m  0.1503\n",
      "     13        \u001b[36m2.5029\u001b[0m        \u001b[32m2.3756\u001b[0m  0.1297\n",
      "     14        \u001b[36m2.5016\u001b[0m        \u001b[32m2.3748\u001b[0m  0.1244\n",
      "     15        \u001b[36m2.5005\u001b[0m        \u001b[32m2.3736\u001b[0m  0.1504\n",
      "     16        \u001b[36m2.4995\u001b[0m        \u001b[32m2.3725\u001b[0m  0.1336\n",
      "     17        \u001b[36m2.4990\u001b[0m        \u001b[32m2.3718\u001b[0m  0.1870\n",
      "     18        \u001b[36m2.4984\u001b[0m        \u001b[32m2.3694\u001b[0m  0.1929\n",
      "     19        \u001b[36m2.4971\u001b[0m        \u001b[32m2.3692\u001b[0m  0.1482\n",
      "     20        \u001b[36m2.4966\u001b[0m        \u001b[32m2.3687\u001b[0m  0.1431\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4407\u001b[0m        \u001b[32m3.0361\u001b[0m  0.1421\n",
      "      2        \u001b[36m2.8539\u001b[0m        \u001b[32m2.7123\u001b[0m  0.1632\n",
      "      3        \u001b[36m2.6629\u001b[0m        \u001b[32m2.6010\u001b[0m  0.1310\n",
      "      4        \u001b[36m2.5965\u001b[0m        \u001b[32m2.5605\u001b[0m  0.1060\n",
      "      5        \u001b[36m2.5679\u001b[0m        \u001b[32m2.5498\u001b[0m  0.1774\n",
      "      6        \u001b[36m2.5533\u001b[0m        \u001b[32m2.5321\u001b[0m  0.1352\n",
      "      7        \u001b[36m2.5439\u001b[0m        \u001b[32m2.5209\u001b[0m  0.1467\n",
      "      8        \u001b[36m2.5379\u001b[0m        \u001b[32m2.5166\u001b[0m  0.1111\n",
      "      9        \u001b[36m2.5337\u001b[0m        \u001b[32m2.5131\u001b[0m  0.1535\n",
      "     10        \u001b[36m2.5306\u001b[0m        \u001b[32m2.5106\u001b[0m  0.1586\n",
      "     11        \u001b[36m2.5286\u001b[0m        \u001b[32m2.5091\u001b[0m  0.1652\n",
      "     12        \u001b[36m2.5268\u001b[0m        \u001b[32m2.5075\u001b[0m  0.1702\n",
      "     13        \u001b[36m2.5253\u001b[0m        \u001b[32m2.5061\u001b[0m  0.1233\n",
      "     14        \u001b[36m2.5241\u001b[0m        \u001b[32m2.5051\u001b[0m  0.1234\n",
      "     15        \u001b[36m2.5232\u001b[0m        \u001b[32m2.5042\u001b[0m  0.1169\n",
      "     16        \u001b[36m2.5224\u001b[0m        \u001b[32m2.5035\u001b[0m  0.1406\n",
      "     17        \u001b[36m2.5218\u001b[0m        \u001b[32m2.5029\u001b[0m  0.1370\n",
      "     18        \u001b[36m2.5212\u001b[0m        \u001b[32m2.5023\u001b[0m  0.1355\n",
      "     19        \u001b[36m2.5207\u001b[0m        \u001b[32m2.5018\u001b[0m  0.1444\n",
      "     20        \u001b[36m2.5203\u001b[0m        \u001b[32m2.5014\u001b[0m  0.1324\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3490\u001b[0m        \u001b[32m3.1096\u001b[0m  0.1041\n",
      "      2        \u001b[36m2.8382\u001b[0m        \u001b[32m2.8518\u001b[0m  0.1171\n",
      "      3        \u001b[36m2.6500\u001b[0m        \u001b[32m2.6174\u001b[0m  0.1192\n",
      "      4        \u001b[36m2.5758\u001b[0m        \u001b[32m2.5592\u001b[0m  0.1109\n",
      "      5        \u001b[36m2.5419\u001b[0m        \u001b[32m2.5409\u001b[0m  0.1189\n",
      "      6        \u001b[36m2.5240\u001b[0m        \u001b[32m2.5294\u001b[0m  0.1356\n",
      "      7        \u001b[36m2.5133\u001b[0m        \u001b[32m2.5231\u001b[0m  0.1231\n",
      "      8        \u001b[36m2.5063\u001b[0m        \u001b[32m2.5180\u001b[0m  0.1361\n",
      "      9        \u001b[36m2.5015\u001b[0m        \u001b[32m2.5140\u001b[0m  0.1219\n",
      "     10        \u001b[36m2.4980\u001b[0m        \u001b[32m2.5113\u001b[0m  0.1063\n",
      "     11        \u001b[36m2.4953\u001b[0m        \u001b[32m2.5096\u001b[0m  0.1219\n",
      "     12        \u001b[36m2.4933\u001b[0m        \u001b[32m2.5080\u001b[0m  0.1205\n",
      "     13        \u001b[36m2.4917\u001b[0m        \u001b[32m2.5066\u001b[0m  0.1575\n",
      "     14        \u001b[36m2.4904\u001b[0m        \u001b[32m2.5057\u001b[0m  0.1364\n",
      "     15        \u001b[36m2.4893\u001b[0m        \u001b[32m2.5048\u001b[0m  0.1555\n",
      "     16        \u001b[36m2.4884\u001b[0m        \u001b[32m2.5041\u001b[0m  0.1168\n",
      "     17        \u001b[36m2.4877\u001b[0m        \u001b[32m2.5034\u001b[0m  0.1039\n",
      "     18        \u001b[36m2.4870\u001b[0m        \u001b[32m2.5029\u001b[0m  0.1246\n",
      "     19        \u001b[36m2.4864\u001b[0m        \u001b[32m2.5025\u001b[0m  0.1220\n",
      "     20        \u001b[36m2.4860\u001b[0m        \u001b[32m2.5020\u001b[0m  0.1043\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3350\u001b[0m        \u001b[32m2.7977\u001b[0m  0.1198\n",
      "      2        \u001b[36m2.8202\u001b[0m        \u001b[32m2.5622\u001b[0m  0.1285\n",
      "      3        \u001b[36m2.6408\u001b[0m        \u001b[32m2.4659\u001b[0m  0.1183\n",
      "      4        \u001b[36m2.5747\u001b[0m        \u001b[32m2.4264\u001b[0m  0.1296\n",
      "      5        \u001b[36m2.5454\u001b[0m        \u001b[32m2.4025\u001b[0m  0.1445\n",
      "      6        \u001b[36m2.5298\u001b[0m        \u001b[32m2.3906\u001b[0m  0.1262\n",
      "      7        \u001b[36m2.5203\u001b[0m        \u001b[32m2.3833\u001b[0m  0.1210\n",
      "      8        \u001b[36m2.5142\u001b[0m        \u001b[32m2.3783\u001b[0m  0.1041\n",
      "      9        \u001b[36m2.5100\u001b[0m        \u001b[32m2.3747\u001b[0m  0.1210\n",
      "     10        \u001b[36m2.5069\u001b[0m        \u001b[32m2.3720\u001b[0m  0.1181\n",
      "     11        \u001b[36m2.5045\u001b[0m        \u001b[32m2.3700\u001b[0m  0.1268\n",
      "     12        \u001b[36m2.5027\u001b[0m        \u001b[32m2.3684\u001b[0m  0.1328\n",
      "     13        \u001b[36m2.5013\u001b[0m        \u001b[32m2.3671\u001b[0m  0.1192\n",
      "     14        \u001b[36m2.5001\u001b[0m        \u001b[32m2.3660\u001b[0m  0.1318\n",
      "     15        \u001b[36m2.4991\u001b[0m        \u001b[32m2.3650\u001b[0m  0.1173\n",
      "     16        \u001b[36m2.4983\u001b[0m        \u001b[32m2.3645\u001b[0m  0.1277\n",
      "     17        \u001b[36m2.4975\u001b[0m        \u001b[32m2.3645\u001b[0m  0.1092\n",
      "     18        \u001b[36m2.4970\u001b[0m        \u001b[32m2.3639\u001b[0m  0.1249\n",
      "     19        \u001b[36m2.4965\u001b[0m        \u001b[32m2.3634\u001b[0m  0.1190\n",
      "     20        \u001b[36m2.4960\u001b[0m        \u001b[32m2.3629\u001b[0m  0.1083\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3614\u001b[0m        \u001b[32m3.0627\u001b[0m  0.1378\n",
      "      2        \u001b[36m2.8455\u001b[0m        \u001b[32m2.7286\u001b[0m  0.1173\n",
      "      3        \u001b[36m2.6667\u001b[0m        \u001b[32m2.6070\u001b[0m  0.1166\n",
      "      4        \u001b[36m2.6007\u001b[0m        \u001b[32m2.5648\u001b[0m  0.1160\n",
      "      5        \u001b[36m2.5707\u001b[0m        \u001b[32m2.5424\u001b[0m  0.1196\n",
      "      6        \u001b[36m2.5550\u001b[0m        \u001b[32m2.5316\u001b[0m  0.1415\n",
      "      7        \u001b[36m2.5455\u001b[0m        \u001b[32m2.5230\u001b[0m  0.1610\n",
      "      8        \u001b[36m2.5390\u001b[0m        \u001b[32m2.5181\u001b[0m  0.1250\n",
      "      9        \u001b[36m2.5347\u001b[0m        \u001b[32m2.5149\u001b[0m  0.1593\n",
      "     10        \u001b[36m2.5315\u001b[0m        \u001b[32m2.5120\u001b[0m  0.1416\n",
      "     11        \u001b[36m2.5292\u001b[0m        \u001b[32m2.5099\u001b[0m  0.1544\n",
      "     12        \u001b[36m2.5274\u001b[0m        \u001b[32m2.5080\u001b[0m  0.1329\n",
      "     13        \u001b[36m2.5259\u001b[0m        \u001b[32m2.5069\u001b[0m  0.1336\n",
      "     14        \u001b[36m2.5247\u001b[0m        \u001b[32m2.5056\u001b[0m  0.1434\n",
      "     15        \u001b[36m2.5237\u001b[0m        \u001b[32m2.5048\u001b[0m  0.1811\n",
      "     16        \u001b[36m2.5229\u001b[0m        \u001b[32m2.5040\u001b[0m  0.1341\n",
      "     17        \u001b[36m2.5223\u001b[0m        \u001b[32m2.5034\u001b[0m  0.1639\n",
      "     18        \u001b[36m2.5217\u001b[0m        \u001b[32m2.5029\u001b[0m  0.1841\n",
      "     19        \u001b[36m2.5212\u001b[0m        \u001b[32m2.5024\u001b[0m  0.1352\n",
      "     20        \u001b[36m2.5207\u001b[0m        \u001b[32m2.5022\u001b[0m  0.1311\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3120\u001b[0m        \u001b[32m3.0150\u001b[0m  0.1344\n",
      "      2        \u001b[36m2.8168\u001b[0m        \u001b[32m2.7015\u001b[0m  0.1137\n",
      "      3        \u001b[36m2.6377\u001b[0m        \u001b[32m2.6131\u001b[0m  0.2110\n",
      "      4        \u001b[36m2.5703\u001b[0m        \u001b[32m2.5566\u001b[0m  0.1224\n",
      "      5        \u001b[36m2.5382\u001b[0m        \u001b[32m2.5395\u001b[0m  0.1239\n",
      "      6        \u001b[36m2.5215\u001b[0m        \u001b[32m2.5251\u001b[0m  0.1712\n",
      "      7        \u001b[36m2.5119\u001b[0m        \u001b[32m2.5245\u001b[0m  0.1517\n",
      "      8        \u001b[36m2.5050\u001b[0m        \u001b[32m2.5180\u001b[0m  0.1551\n",
      "      9        \u001b[36m2.5004\u001b[0m        \u001b[32m2.5116\u001b[0m  0.1861\n",
      "     10        \u001b[36m2.4971\u001b[0m        \u001b[32m2.5098\u001b[0m  0.1877\n",
      "     11        \u001b[36m2.4947\u001b[0m        \u001b[32m2.5078\u001b[0m  0.1535\n",
      "     12        \u001b[36m2.4927\u001b[0m        \u001b[32m2.5072\u001b[0m  0.2011\n",
      "     13        \u001b[36m2.4913\u001b[0m        \u001b[32m2.5060\u001b[0m  0.1291\n",
      "     14        \u001b[36m2.4900\u001b[0m        \u001b[32m2.5044\u001b[0m  0.1459\n",
      "     15        \u001b[36m2.4890\u001b[0m        2.5047  0.1229\n",
      "     16        \u001b[36m2.4881\u001b[0m        \u001b[32m2.5041\u001b[0m  0.1369\n",
      "     17        \u001b[36m2.4874\u001b[0m        \u001b[32m2.5032\u001b[0m  0.1217\n",
      "     18        \u001b[36m2.4868\u001b[0m        \u001b[32m2.5025\u001b[0m  0.1563\n",
      "     19        \u001b[36m2.4863\u001b[0m        \u001b[32m2.5024\u001b[0m  0.1675\n",
      "     20        \u001b[36m2.4858\u001b[0m        \u001b[32m2.5019\u001b[0m  0.1544\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=&lt;class &#x27;skorch.net.NeuralNet&#x27;&gt;[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)\n",
       "    (...\n",
       "        [   3.8125,   49.0000,    4.4735,  ...,    1.7381,   33.7700,\n",
       "         -118.1600],\n",
       "        [   4.1563,    4.0000,    5.6458,  ...,    2.7232,   34.6600,\n",
       "         -120.4800],\n",
       "        ...,\n",
       "        [   2.9344,   36.0000,    3.9867,  ...,    3.3321,   34.0300,\n",
       "         -118.3800],\n",
       "        [   5.7192,   15.0000,    6.3953,  ...,    3.1789,   37.5800,\n",
       "         -121.9600],\n",
       "        [   2.5755,   52.0000,    3.4026,  ...,    2.1087,   37.7700,\n",
       "         -122.4200]])],\n",
       "                         &#x27;module__n_neurons&#x27;: [10, 50, 100]},\n",
       "             refit=False, scoring=&#x27;neg_mean_squared_error&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=&lt;class &#x27;skorch.net.NeuralNet&#x27;&gt;[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)\n",
       "    (...\n",
       "        [   3.8125,   49.0000,    4.4735,  ...,    1.7381,   33.7700,\n",
       "         -118.1600],\n",
       "        [   4.1563,    4.0000,    5.6458,  ...,    2.7232,   34.6600,\n",
       "         -120.4800],\n",
       "        ...,\n",
       "        [   2.9344,   36.0000,    3.9867,  ...,    3.3321,   34.0300,\n",
       "         -118.3800],\n",
       "        [   5.7192,   15.0000,    6.3953,  ...,    3.1789,   37.5800,\n",
       "         -121.9600],\n",
       "        [   2.5755,   52.0000,    3.4026,  ...,    2.1087,   37.7700,\n",
       "         -122.4200]])],\n",
       "                         &#x27;module__n_neurons&#x27;: [10, 50, 100]},\n",
       "             refit=False, scoring=&#x27;neg_mean_squared_error&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: NeuralNet</label><div class=\"sk-toggleable__content\"><pre>&lt;class &#x27;skorch.net.NeuralNet&#x27;&gt;[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)\n",
       "    (act2): Sigmoid()\n",
       "  ),\n",
       ")</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NeuralNet</label><div class=\"sk-toggleable__content\"><pre>&lt;class &#x27;skorch.net.NeuralNet&#x27;&gt;[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)\n",
       "    (act2): Sigmoid()\n",
       "  ),\n",
       ")</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=<class 'skorch.net.NeuralNet'>[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)\n",
       "    (...\n",
       "        [   3.8125,   49.0000,    4.4735,  ...,    1.7381,   33.7700,\n",
       "         -118.1600],\n",
       "        [   4.1563,    4.0000,    5.6458,  ...,    2.7232,   34.6600,\n",
       "         -120.4800],\n",
       "        ...,\n",
       "        [   2.9344,   36.0000,    3.9867,  ...,    3.3321,   34.0300,\n",
       "         -118.3800],\n",
       "        [   5.7192,   15.0000,    6.3953,  ...,    3.1789,   37.5800,\n",
       "         -121.9600],\n",
       "        [   2.5755,   52.0000,    3.4026,  ...,    2.1087,   37.7700,\n",
       "         -122.4200]])],\n",
       "                         'module__n_neurons': [10, 50, 100]},\n",
       "             refit=False, scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d183a8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.492409865061442"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115dca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.02,\n",
       " 'max_epochs': 20,\n",
       " 'module__X': tensor([[   3.2596,   33.0000,    5.0177,  ...,    3.6918,   32.7100,\n",
       "          -117.0300],\n",
       "         [   3.8125,   49.0000,    4.4735,  ...,    1.7381,   33.7700,\n",
       "          -118.1600],\n",
       "         [   4.1563,    4.0000,    5.6458,  ...,    2.7232,   34.6600,\n",
       "          -120.4800],\n",
       "         ...,\n",
       "         [   2.9344,   36.0000,    3.9867,  ...,    3.3321,   34.0300,\n",
       "          -118.3800],\n",
       "         [   5.7192,   15.0000,    6.3953,  ...,    3.1789,   37.5800,\n",
       "          -121.9600],\n",
       "         [   2.5755,   52.0000,    3.4026,  ...,    2.1087,   37.7700,\n",
       "          -122.4200]]),\n",
       " 'module__n_neurons': 10}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e511b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(estimator=&lt;class &#x27;skorch.net.NeuralNet&#x27;&gt;[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)...\n",
       "                                        &#x27;module__n_neurons&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])},\n",
       "                   refit=False, scoring=&#x27;neg_mean_squared_error&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(estimator=&lt;class &#x27;skorch.net.NeuralNet&#x27;&gt;[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)...\n",
       "                                        &#x27;module__n_neurons&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])},\n",
       "                   refit=False, scoring=&#x27;neg_mean_squared_error&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: NeuralNet</label><div class=\"sk-toggleable__content\"><pre>&lt;class &#x27;skorch.net.NeuralNet&#x27;&gt;[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)\n",
       "    (act2): Sigmoid()\n",
       "  ),\n",
       ")</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NeuralNet</label><div class=\"sk-toggleable__content\"><pre>&lt;class &#x27;skorch.net.NeuralNet&#x27;&gt;[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)\n",
       "    (act2): Sigmoid()\n",
       "  ),\n",
       ")</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(estimator=<class 'skorch.net.NeuralNet'>[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)...\n",
       "                                        'module__n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])},\n",
       "                   refit=False, scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "param_distribs = {\n",
    "    \"lr\": reciprocal(3e-4, 3e-2),\n",
    "    \"module__n_neurons\": np.arange(1, 100),\n",
    "    \"module__X\": [X_train]\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(estimator=model, param_distributions=param_distribs, n_iter=10, \n",
    "                                   refit=False, scoring='neg_mean_squared_error', verbose=1)\n",
    "rnd_search_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b10c232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.2990\u001b[0m        \u001b[32m2.9619\u001b[0m  0.2194\n",
      "      2        \u001b[36m2.7902\u001b[0m        \u001b[32m2.7093\u001b[0m  0.1960\n",
      "      3        \u001b[36m2.6150\u001b[0m        \u001b[32m2.5878\u001b[0m  0.2318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        \u001b[36m2.5505\u001b[0m        \u001b[32m2.5479\u001b[0m  0.1769\n",
      "      5        \u001b[36m2.5213\u001b[0m        \u001b[32m2.5256\u001b[0m  0.1901\n",
      "      6        \u001b[36m2.5060\u001b[0m        \u001b[32m2.5122\u001b[0m  0.1726\n",
      "      7        \u001b[36m2.4962\u001b[0m        \u001b[32m2.5060\u001b[0m  0.1540\n",
      "      8        \u001b[36m2.4899\u001b[0m        \u001b[32m2.5005\u001b[0m  0.1437\n",
      "      9        \u001b[36m2.4855\u001b[0m        \u001b[32m2.4999\u001b[0m  0.1716\n",
      "     10        \u001b[36m2.4823\u001b[0m        \u001b[32m2.4954\u001b[0m  0.1524\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3076\u001b[0m        \u001b[32m3.0800\u001b[0m  0.1736\n",
      "      2        \u001b[36m2.8353\u001b[0m        \u001b[32m2.8517\u001b[0m  0.1568\n",
      "      3        \u001b[36m2.6654\u001b[0m        \u001b[32m2.7767\u001b[0m  0.1512\n",
      "      4        \u001b[36m2.5980\u001b[0m        \u001b[32m2.7102\u001b[0m  0.1768\n",
      "      5        \u001b[36m2.5650\u001b[0m        \u001b[32m2.6418\u001b[0m  0.1670\n",
      "      6        \u001b[36m2.5420\u001b[0m        \u001b[32m2.6234\u001b[0m  0.1638\n",
      "      7        \u001b[36m2.5276\u001b[0m        \u001b[32m2.6108\u001b[0m  0.1691\n",
      "      8        \u001b[36m2.5163\u001b[0m        \u001b[32m2.5998\u001b[0m  0.2092\n",
      "      9        \u001b[36m2.5079\u001b[0m        \u001b[32m2.5950\u001b[0m  0.1683\n",
      "     10        \u001b[36m2.4990\u001b[0m        \u001b[32m2.5908\u001b[0m  0.1742\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3041\u001b[0m        \u001b[32m3.0485\u001b[0m  0.1457\n",
      "      2        \u001b[36m2.8076\u001b[0m        \u001b[32m2.8561\u001b[0m  0.1923\n",
      "      3        \u001b[36m2.6238\u001b[0m        \u001b[32m2.6962\u001b[0m  0.1891\n",
      "      4        \u001b[36m2.5530\u001b[0m        \u001b[32m2.6402\u001b[0m  0.1740\n",
      "      5        \u001b[36m2.5204\u001b[0m        \u001b[32m2.6195\u001b[0m  0.1554\n",
      "      6        \u001b[36m2.5032\u001b[0m        \u001b[32m2.5996\u001b[0m  0.2139\n",
      "      7        \u001b[36m2.4931\u001b[0m        \u001b[32m2.5968\u001b[0m  0.1397\n",
      "      8        \u001b[36m2.4865\u001b[0m        \u001b[32m2.5912\u001b[0m  0.1408\n",
      "      9        \u001b[36m2.4819\u001b[0m        \u001b[32m2.5873\u001b[0m  0.1880\n",
      "     10        \u001b[36m2.4786\u001b[0m        \u001b[32m2.5859\u001b[0m  0.1817\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4089\u001b[0m        \u001b[32m2.9585\u001b[0m  0.2131\n",
      "      2        \u001b[36m2.8523\u001b[0m        \u001b[32m2.8217\u001b[0m  0.1807\n",
      "      3        \u001b[36m2.6566\u001b[0m        \u001b[32m2.6947\u001b[0m  0.1512\n",
      "      4        \u001b[36m2.5846\u001b[0m        \u001b[32m2.6553\u001b[0m  0.1439\n",
      "      5        \u001b[36m2.5529\u001b[0m        \u001b[32m2.6286\u001b[0m  0.1393\n",
      "      6        \u001b[36m2.5363\u001b[0m        \u001b[32m2.6129\u001b[0m  0.1591\n",
      "      7        \u001b[36m2.5264\u001b[0m        \u001b[32m2.5959\u001b[0m  0.1779\n",
      "      8        \u001b[36m2.5200\u001b[0m        \u001b[32m2.5908\u001b[0m  0.1759\n",
      "      9        \u001b[36m2.5155\u001b[0m        \u001b[32m2.5888\u001b[0m  0.2035\n",
      "     10        \u001b[36m2.5124\u001b[0m        \u001b[32m2.5861\u001b[0m  0.1704\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.3217\u001b[0m        \u001b[32m3.0721\u001b[0m  0.1965\n",
      "      2        \u001b[36m2.8030\u001b[0m        \u001b[32m2.8540\u001b[0m  0.1955\n",
      "      3        \u001b[36m2.6245\u001b[0m        \u001b[32m2.7456\u001b[0m  0.1639\n",
      "      4        \u001b[36m2.5575\u001b[0m        \u001b[32m2.6504\u001b[0m  0.1446\n",
      "      5        \u001b[36m2.5268\u001b[0m        \u001b[32m2.6255\u001b[0m  0.1349\n",
      "      6        \u001b[36m2.5094\u001b[0m        \u001b[32m2.6158\u001b[0m  0.1709\n",
      "      7        \u001b[36m2.4946\u001b[0m        \u001b[32m2.6045\u001b[0m  0.1542\n",
      "      8        \u001b[36m2.4791\u001b[0m        \u001b[32m2.5949\u001b[0m  0.2238\n",
      "      9        \u001b[36m2.4728\u001b[0m        \u001b[32m2.5900\u001b[0m  0.2685\n",
      "     10        \u001b[36m2.4691\u001b[0m        \u001b[32m2.5879\u001b[0m  0.1978\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.8253\u001b[0m        \u001b[32m3.8450\u001b[0m  0.1685\n",
      "      2        \u001b[36m3.7915\u001b[0m        \u001b[32m3.8105\u001b[0m  0.1802\n",
      "      3        \u001b[36m3.7582\u001b[0m        \u001b[32m3.7766\u001b[0m  0.1587\n",
      "      4        \u001b[36m3.7253\u001b[0m        \u001b[32m3.7431\u001b[0m  0.1628\n",
      "      5        \u001b[36m3.6928\u001b[0m        \u001b[32m3.7101\u001b[0m  0.1962\n",
      "      6        \u001b[36m3.6608\u001b[0m        \u001b[32m3.6774\u001b[0m  0.2022\n",
      "      7        \u001b[36m3.6293\u001b[0m        \u001b[32m3.6452\u001b[0m  0.1463\n",
      "      8        \u001b[36m3.5982\u001b[0m        \u001b[32m3.6126\u001b[0m  0.1922\n",
      "      9        \u001b[36m3.5675\u001b[0m        \u001b[32m3.5751\u001b[0m  0.1511\n",
      "     10        \u001b[36m3.5346\u001b[0m        \u001b[32m3.4687\u001b[0m  0.1539\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6823\u001b[0m        \u001b[32m3.7688\u001b[0m  0.1317\n",
      "      2        \u001b[36m3.6539\u001b[0m        \u001b[32m3.7409\u001b[0m  0.1446\n",
      "      3        \u001b[36m3.6262\u001b[0m        \u001b[32m3.7069\u001b[0m  0.1799\n",
      "      4        \u001b[36m3.5996\u001b[0m        \u001b[32m3.6796\u001b[0m  0.1432\n",
      "      5        \u001b[36m3.5739\u001b[0m        \u001b[32m3.6542\u001b[0m  0.1496\n",
      "      6        \u001b[36m3.5497\u001b[0m        \u001b[32m3.6329\u001b[0m  0.1801\n",
      "      7        \u001b[36m3.5262\u001b[0m        \u001b[32m3.6070\u001b[0m  0.1943\n",
      "      8        \u001b[36m3.5033\u001b[0m        \u001b[32m3.5828\u001b[0m  0.1585\n",
      "      9        \u001b[36m3.4806\u001b[0m        \u001b[32m3.5568\u001b[0m  0.1872\n",
      "     10        \u001b[36m3.4580\u001b[0m        \u001b[32m3.5370\u001b[0m  0.1679\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7306\u001b[0m        \u001b[32m3.8504\u001b[0m  0.1198\n",
      "      2        \u001b[36m3.6986\u001b[0m        \u001b[32m3.8179\u001b[0m  0.1646\n",
      "      3        \u001b[36m3.6672\u001b[0m        \u001b[32m3.7861\u001b[0m  0.1717\n",
      "      4        \u001b[36m3.6364\u001b[0m        \u001b[32m3.7544\u001b[0m  0.1344\n",
      "      5        \u001b[36m3.6061\u001b[0m        \u001b[32m3.7250\u001b[0m  0.1383\n",
      "      6        \u001b[36m3.5763\u001b[0m        \u001b[32m3.6951\u001b[0m  0.1500\n",
      "      7        \u001b[36m3.5472\u001b[0m        \u001b[32m3.6659\u001b[0m  0.1552\n",
      "      8        \u001b[36m3.5186\u001b[0m        \u001b[32m3.6371\u001b[0m  0.1279\n",
      "      9        \u001b[36m3.4906\u001b[0m        \u001b[32m3.6090\u001b[0m  0.1420\n",
      "     10        \u001b[36m3.4632\u001b[0m        \u001b[32m3.5812\u001b[0m  0.1694\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.8770\u001b[0m        \u001b[32m3.9408\u001b[0m  0.1255\n",
      "      2        \u001b[36m3.8400\u001b[0m        \u001b[32m3.9076\u001b[0m  0.1358\n",
      "      3        \u001b[36m3.8044\u001b[0m        \u001b[32m3.8567\u001b[0m  0.1776\n",
      "      4        \u001b[36m3.7650\u001b[0m        \u001b[32m3.8322\u001b[0m  0.1488\n",
      "      5        \u001b[36m3.7310\u001b[0m        \u001b[32m3.7867\u001b[0m  0.1413\n",
      "      6        \u001b[36m3.6988\u001b[0m        \u001b[32m3.6866\u001b[0m  0.1370\n",
      "      7        \u001b[36m3.6670\u001b[0m        3.7871  0.1539\n",
      "      8        \u001b[36m3.6360\u001b[0m        \u001b[32m3.6006\u001b[0m  0.1474\n",
      "      9        \u001b[36m3.6043\u001b[0m        3.7195  0.1876\n",
      "     10        \u001b[36m3.5745\u001b[0m        3.6234  0.1525\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.8615\u001b[0m        \u001b[32m3.9704\u001b[0m  0.1752\n",
      "      2        \u001b[36m3.8260\u001b[0m        \u001b[32m3.9534\u001b[0m  0.1321\n",
      "      3        \u001b[36m3.7952\u001b[0m        \u001b[32m3.9212\u001b[0m  0.1254\n",
      "      4        \u001b[36m3.7622\u001b[0m        \u001b[32m3.8859\u001b[0m  0.2421\n",
      "      5        \u001b[36m3.7300\u001b[0m        \u001b[32m3.8524\u001b[0m  0.1297\n",
      "      6        \u001b[36m3.6982\u001b[0m        \u001b[32m3.8187\u001b[0m  0.1923\n",
      "      7        \u001b[36m3.6664\u001b[0m        \u001b[32m3.7828\u001b[0m  0.2158\n",
      "      8        \u001b[36m3.6348\u001b[0m        \u001b[32m3.7673\u001b[0m  0.1653\n",
      "      9        \u001b[36m3.6044\u001b[0m        \u001b[32m3.7560\u001b[0m  0.1415\n",
      "     10        \u001b[36m3.5794\u001b[0m        \u001b[32m3.7023\u001b[0m  0.1807\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7568\u001b[0m        \u001b[32m3.5974\u001b[0m  0.1464\n",
      "      2        \u001b[36m3.5228\u001b[0m        \u001b[32m3.4237\u001b[0m  0.1373\n",
      "      3        \u001b[36m3.3200\u001b[0m        \u001b[32m3.2501\u001b[0m  0.1275\n",
      "      4        \u001b[36m3.1477\u001b[0m        \u001b[32m3.0868\u001b[0m  0.1354\n",
      "      5        \u001b[36m3.0083\u001b[0m        \u001b[32m2.9668\u001b[0m  0.1347\n",
      "      6        \u001b[36m2.8977\u001b[0m        \u001b[32m2.8673\u001b[0m  0.1242\n",
      "      7        \u001b[36m2.8121\u001b[0m        \u001b[32m2.7949\u001b[0m  0.1443\n",
      "      8        \u001b[36m2.7467\u001b[0m        2.7997  0.1649\n",
      "      9        \u001b[36m2.6963\u001b[0m        \u001b[32m2.6860\u001b[0m  0.1519\n",
      "     10        \u001b[36m2.6579\u001b[0m        \u001b[32m2.6399\u001b[0m  0.1430\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7892\u001b[0m        \u001b[32m3.7872\u001b[0m  0.1610\n",
      "      2        \u001b[36m3.5509\u001b[0m        \u001b[32m3.5549\u001b[0m  0.1333\n",
      "      3        \u001b[36m3.3415\u001b[0m        \u001b[32m3.3839\u001b[0m  0.1396\n",
      "      4        \u001b[36m3.1690\u001b[0m        \u001b[32m3.2497\u001b[0m  0.1309\n",
      "      5        \u001b[36m3.0319\u001b[0m        \u001b[32m3.0494\u001b[0m  0.1387\n",
      "      6        \u001b[36m2.9194\u001b[0m        \u001b[32m2.9819\u001b[0m  0.1779\n",
      "      7        \u001b[36m2.8376\u001b[0m        \u001b[32m2.8450\u001b[0m  0.1428\n",
      "      8        \u001b[36m2.7738\u001b[0m        \u001b[32m2.8287\u001b[0m  0.1373\n",
      "      9        \u001b[36m2.7266\u001b[0m        \u001b[32m2.7801\u001b[0m  0.2212\n",
      "     10        \u001b[36m2.6941\u001b[0m        \u001b[32m2.7714\u001b[0m  0.1830\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6058\u001b[0m        \u001b[32m3.6186\u001b[0m  0.1206\n",
      "      2        \u001b[36m3.3793\u001b[0m        \u001b[32m3.4042\u001b[0m  0.2892\n",
      "      3        \u001b[36m3.1959\u001b[0m        \u001b[32m3.2284\u001b[0m  0.1335\n",
      "      4        \u001b[36m3.0451\u001b[0m        \u001b[32m3.1574\u001b[0m  0.1788\n",
      "      5        \u001b[36m2.9291\u001b[0m        \u001b[32m2.9907\u001b[0m  0.1431\n",
      "      6        \u001b[36m2.8420\u001b[0m        \u001b[32m2.9235\u001b[0m  0.1441\n",
      "      7        \u001b[36m2.7746\u001b[0m        \u001b[32m2.8598\u001b[0m  0.1314\n",
      "      8        \u001b[36m2.7222\u001b[0m        \u001b[32m2.8119\u001b[0m  0.1368\n",
      "      9        \u001b[36m2.6825\u001b[0m        \u001b[32m2.7753\u001b[0m  0.1514\n",
      "     10        \u001b[36m2.6514\u001b[0m        \u001b[32m2.7460\u001b[0m  0.1430\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6186\u001b[0m        \u001b[32m3.6006\u001b[0m  0.1586\n",
      "      2        \u001b[36m3.4173\u001b[0m        \u001b[32m3.4149\u001b[0m  0.1914\n",
      "      3        \u001b[36m3.2491\u001b[0m        \u001b[32m3.2610\u001b[0m  0.1209\n",
      "      4        \u001b[36m3.1109\u001b[0m        \u001b[32m3.1357\u001b[0m  0.1534\n",
      "      5        \u001b[36m2.9989\u001b[0m        \u001b[32m3.0344\u001b[0m  0.1732\n",
      "      6        \u001b[36m2.9092\u001b[0m        \u001b[32m2.9539\u001b[0m  0.1615\n",
      "      7        \u001b[36m2.8374\u001b[0m        \u001b[32m2.8914\u001b[0m  0.1813\n",
      "      8        \u001b[36m2.7783\u001b[0m        \u001b[32m2.8430\u001b[0m  0.1773\n",
      "      9        \u001b[36m2.7343\u001b[0m        \u001b[32m2.8090\u001b[0m  0.1482\n",
      "     10        \u001b[36m2.6987\u001b[0m        \u001b[32m2.7657\u001b[0m  0.1764\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6111\u001b[0m        \u001b[32m3.5947\u001b[0m  0.1360\n",
      "      2        \u001b[36m3.4035\u001b[0m        \u001b[32m3.3323\u001b[0m  0.1208\n",
      "      3        \u001b[36m3.2238\u001b[0m        \u001b[32m3.0602\u001b[0m  0.1274\n",
      "      4        \u001b[36m3.0727\u001b[0m        3.1381  0.1412\n",
      "      5        \u001b[36m2.9513\u001b[0m        \u001b[32m3.0577\u001b[0m  0.1779\n",
      "      6        \u001b[36m2.8561\u001b[0m        \u001b[32m2.8739\u001b[0m  0.1453\n",
      "      7        \u001b[36m2.7819\u001b[0m        2.9169  0.1486\n",
      "      8        \u001b[36m2.7225\u001b[0m        \u001b[32m2.7370\u001b[0m  0.1442\n",
      "      9        \u001b[36m2.6781\u001b[0m        \u001b[32m2.7275\u001b[0m  0.1248\n",
      "     10        \u001b[36m2.6419\u001b[0m        2.7767  0.1553\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7287\u001b[0m        \u001b[32m3.7243\u001b[0m  0.1514\n",
      "      2        \u001b[36m3.6694\u001b[0m        \u001b[32m3.7012\u001b[0m  0.1830\n",
      "      3        \u001b[36m3.6208\u001b[0m        \u001b[32m3.6550\u001b[0m  0.1464\n",
      "      4        \u001b[36m3.5762\u001b[0m        \u001b[32m3.6109\u001b[0m  0.1473\n",
      "      5        \u001b[36m3.5339\u001b[0m        \u001b[32m3.5554\u001b[0m  0.1723\n",
      "      6        \u001b[36m3.4930\u001b[0m        \u001b[32m3.5127\u001b[0m  0.2313\n",
      "      7        \u001b[36m3.4534\u001b[0m        \u001b[32m3.4689\u001b[0m  0.1901\n",
      "      8        \u001b[36m3.4152\u001b[0m        \u001b[32m3.4282\u001b[0m  0.2031\n",
      "      9        \u001b[36m3.3782\u001b[0m        \u001b[32m3.3897\u001b[0m  0.2032\n",
      "     10        \u001b[36m3.3424\u001b[0m        \u001b[32m3.3535\u001b[0m  0.1779\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7382\u001b[0m        \u001b[32m3.8450\u001b[0m  0.1935\n",
      "      2        \u001b[36m3.6900\u001b[0m        \u001b[32m3.8027\u001b[0m  0.1825\n",
      "      3        \u001b[36m3.6455\u001b[0m        \u001b[32m3.7576\u001b[0m  0.1344\n",
      "      4        \u001b[36m3.6020\u001b[0m        \u001b[32m3.7176\u001b[0m  0.1670\n",
      "      5        \u001b[36m3.5596\u001b[0m        \u001b[32m3.6772\u001b[0m  0.1697\n",
      "      6        \u001b[36m3.5181\u001b[0m        \u001b[32m3.6432\u001b[0m  0.2002\n",
      "      7        \u001b[36m3.4770\u001b[0m        \u001b[32m3.5971\u001b[0m  0.1606\n",
      "      8        \u001b[36m3.4348\u001b[0m        \u001b[32m3.5230\u001b[0m  0.1753\n",
      "      9        \u001b[36m3.3940\u001b[0m        \u001b[32m3.4955\u001b[0m  0.1345\n",
      "     10        \u001b[36m3.3545\u001b[0m        \u001b[32m3.4561\u001b[0m  0.1986\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.8103\u001b[0m        \u001b[32m3.9513\u001b[0m  0.1380\n",
      "      2        \u001b[36m3.7580\u001b[0m        \u001b[32m3.8043\u001b[0m  0.1749\n",
      "      3        \u001b[36m3.7106\u001b[0m        3.8254  0.1806\n",
      "      4        \u001b[36m3.6637\u001b[0m        3.8225  0.1903\n",
      "      5        \u001b[36m3.6214\u001b[0m        \u001b[32m3.7215\u001b[0m  0.1537\n",
      "      6        \u001b[36m3.5772\u001b[0m        \u001b[32m3.6210\u001b[0m  0.1792\n",
      "      7        \u001b[36m3.5361\u001b[0m        \u001b[32m3.5935\u001b[0m  0.1743\n",
      "      8        \u001b[36m3.4961\u001b[0m        \u001b[32m3.4758\u001b[0m  0.2055\n",
      "      9        \u001b[36m3.4594\u001b[0m        3.5734  0.1617\n",
      "     10        \u001b[36m3.4186\u001b[0m        3.5822  0.1699\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.8376\u001b[0m        \u001b[32m3.9086\u001b[0m  0.1695\n",
      "      2        \u001b[36m3.7903\u001b[0m        \u001b[32m3.8330\u001b[0m  0.1374\n",
      "      3        \u001b[36m3.7435\u001b[0m        \u001b[32m3.7738\u001b[0m  0.1837\n",
      "      4        \u001b[36m3.6981\u001b[0m        3.7793  0.1977\n",
      "      5        \u001b[36m3.6542\u001b[0m        \u001b[32m3.7307\u001b[0m  0.1797\n",
      "      6        \u001b[36m3.6117\u001b[0m        \u001b[32m3.6390\u001b[0m  0.1743\n",
      "      7        \u001b[36m3.5706\u001b[0m        \u001b[32m3.5689\u001b[0m  0.1993\n",
      "      8        \u001b[36m3.5305\u001b[0m        \u001b[32m3.5391\u001b[0m  0.1636\n",
      "      9        \u001b[36m3.4906\u001b[0m        3.5891  0.1657\n",
      "     10        \u001b[36m3.4525\u001b[0m        \u001b[32m3.5199\u001b[0m  0.1727\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6761\u001b[0m        \u001b[32m3.7901\u001b[0m  0.1433\n",
      "      2        \u001b[36m3.6277\u001b[0m        \u001b[32m3.7401\u001b[0m  0.2198\n",
      "      3        \u001b[36m3.5824\u001b[0m        \u001b[32m3.6977\u001b[0m  0.2328\n",
      "      4        \u001b[36m3.5388\u001b[0m        \u001b[32m3.6550\u001b[0m  0.1800\n",
      "      5        \u001b[36m3.4965\u001b[0m        \u001b[32m3.6083\u001b[0m  0.1857\n",
      "      6        \u001b[36m3.4562\u001b[0m        \u001b[32m3.5660\u001b[0m  0.1589\n",
      "      7        \u001b[36m3.4168\u001b[0m        \u001b[32m3.5256\u001b[0m  0.1954\n",
      "      8        \u001b[36m3.3790\u001b[0m        \u001b[32m3.4872\u001b[0m  0.1557\n",
      "      9        \u001b[36m3.3427\u001b[0m        \u001b[32m3.4495\u001b[0m  0.2079\n",
      "     10        \u001b[36m3.3073\u001b[0m        \u001b[32m3.4126\u001b[0m  0.1417\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6213\u001b[0m        \u001b[32m3.5252\u001b[0m  0.1612\n",
      "      2        \u001b[36m3.4247\u001b[0m        \u001b[32m3.3469\u001b[0m  0.2289\n",
      "      3        \u001b[36m3.2598\u001b[0m        \u001b[32m3.1518\u001b[0m  0.1964\n",
      "      4        \u001b[36m3.1212\u001b[0m        \u001b[32m3.0412\u001b[0m  0.2170\n",
      "      5        \u001b[36m3.0113\u001b[0m        \u001b[32m2.9555\u001b[0m  0.1821\n",
      "      6        \u001b[36m2.9214\u001b[0m        3.0325  0.1798\n",
      "      7        \u001b[36m2.8498\u001b[0m        \u001b[32m2.8109\u001b[0m  0.1996\n",
      "      8        \u001b[36m2.7936\u001b[0m        2.8227  0.1710\n",
      "      9        \u001b[36m2.7489\u001b[0m        \u001b[32m2.7311\u001b[0m  0.1723\n",
      "     10        \u001b[36m2.7145\u001b[0m        \u001b[32m2.6993\u001b[0m  0.1956\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7166\u001b[0m        \u001b[32m3.5040\u001b[0m  0.1711\n",
      "      2        \u001b[36m3.5345\u001b[0m        3.5857  0.1936\n",
      "      3        \u001b[36m3.3927\u001b[0m        3.5098  0.2231\n",
      "      4        \u001b[36m3.2625\u001b[0m        \u001b[32m3.2970\u001b[0m  0.1963\n",
      "      5        \u001b[36m3.1399\u001b[0m        \u001b[32m3.0140\u001b[0m  0.2428\n",
      "      6        \u001b[36m3.0331\u001b[0m        3.0763  0.2016\n",
      "      7        \u001b[36m2.9337\u001b[0m        \u001b[32m2.9979\u001b[0m  0.2029\n",
      "      8        \u001b[36m2.8551\u001b[0m        \u001b[32m2.9289\u001b[0m  0.2156\n",
      "      9        \u001b[36m2.7929\u001b[0m        \u001b[32m2.8706\u001b[0m  0.2314\n",
      "     10        \u001b[36m2.7402\u001b[0m        \u001b[32m2.8226\u001b[0m  0.1744\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6651\u001b[0m        \u001b[32m3.6817\u001b[0m  0.1882\n",
      "      2        \u001b[36m3.5130\u001b[0m        \u001b[32m3.5534\u001b[0m  0.1541\n",
      "      3        \u001b[36m3.4082\u001b[0m        \u001b[32m3.4578\u001b[0m  0.1767\n",
      "      4        \u001b[36m3.3197\u001b[0m        \u001b[32m3.4015\u001b[0m  0.1859\n",
      "      5        \u001b[36m3.2360\u001b[0m        3.4719  0.1744\n",
      "      6        \u001b[36m3.1524\u001b[0m        \u001b[32m3.2614\u001b[0m  0.1712\n",
      "      7        \u001b[36m3.0608\u001b[0m        3.2622  0.1876\n",
      "      8        \u001b[36m2.9618\u001b[0m        \u001b[32m2.9613\u001b[0m  0.1775\n",
      "      9        \u001b[36m2.8949\u001b[0m        \u001b[32m2.9512\u001b[0m  0.1977\n",
      "     10        \u001b[36m2.8160\u001b[0m        \u001b[32m2.8875\u001b[0m  0.1686\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7634\u001b[0m        \u001b[32m3.7122\u001b[0m  0.1667\n",
      "      2        \u001b[36m3.5232\u001b[0m        \u001b[32m3.4819\u001b[0m  0.1642\n",
      "      3        \u001b[36m3.3240\u001b[0m        \u001b[32m3.2914\u001b[0m  0.2201\n",
      "      4        \u001b[36m3.1563\u001b[0m        \u001b[32m3.1493\u001b[0m  0.2033\n",
      "      5        \u001b[36m3.0334\u001b[0m        \u001b[32m3.0513\u001b[0m  0.1915\n",
      "      6        \u001b[36m2.9331\u001b[0m        \u001b[32m2.9655\u001b[0m  0.1811\n",
      "      7        \u001b[36m2.8659\u001b[0m        \u001b[32m2.9033\u001b[0m  0.1902\n",
      "      8        \u001b[36m2.8085\u001b[0m        \u001b[32m2.8896\u001b[0m  0.2158\n",
      "      9        \u001b[36m2.7656\u001b[0m        \u001b[32m2.8149\u001b[0m  0.1914\n",
      "     10        \u001b[36m2.7336\u001b[0m        \u001b[32m2.7983\u001b[0m  0.2171\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7420\u001b[0m        \u001b[32m3.7842\u001b[0m  0.1850\n",
      "      2        \u001b[36m3.5536\u001b[0m        \u001b[32m3.6108\u001b[0m  0.2197\n",
      "      3        \u001b[36m3.3974\u001b[0m        \u001b[32m3.4574\u001b[0m  0.2362\n",
      "      4        \u001b[36m3.2523\u001b[0m        \u001b[32m3.3296\u001b[0m  0.2054\n",
      "      5        \u001b[36m3.1114\u001b[0m        \u001b[32m3.1826\u001b[0m  0.2473\n",
      "      6        \u001b[36m2.9968\u001b[0m        \u001b[32m3.0814\u001b[0m  0.2150\n",
      "      7        \u001b[36m2.9006\u001b[0m        \u001b[32m2.9872\u001b[0m  0.2021\n",
      "      8        \u001b[36m2.8234\u001b[0m        \u001b[32m2.8981\u001b[0m  0.1716\n",
      "      9        \u001b[36m2.7631\u001b[0m        \u001b[32m2.8512\u001b[0m  0.2623\n",
      "     10        \u001b[36m2.7144\u001b[0m        \u001b[32m2.8223\u001b[0m  0.2463\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7325\u001b[0m        \u001b[32m3.5518\u001b[0m  0.2247\n",
      "      2        \u001b[36m3.5702\u001b[0m        \u001b[32m3.5440\u001b[0m  0.1874\n",
      "      3        \u001b[36m3.4271\u001b[0m        \u001b[32m3.4094\u001b[0m  0.2408\n",
      "      4        \u001b[36m3.2983\u001b[0m        \u001b[32m3.2660\u001b[0m  0.2938\n",
      "      5        \u001b[36m3.1850\u001b[0m        \u001b[32m3.1778\u001b[0m  0.2097\n",
      "      6        \u001b[36m3.0853\u001b[0m        \u001b[32m3.0895\u001b[0m  0.1759\n",
      "      7        \u001b[36m2.9972\u001b[0m        \u001b[32m3.0085\u001b[0m  0.2137\n",
      "      8        \u001b[36m2.9224\u001b[0m        \u001b[32m2.9098\u001b[0m  0.2057\n",
      "      9        \u001b[36m2.8597\u001b[0m        \u001b[32m2.8497\u001b[0m  0.2415\n",
      "     10        \u001b[36m2.8067\u001b[0m        \u001b[32m2.7982\u001b[0m  0.2057\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7911\u001b[0m        \u001b[32m3.8646\u001b[0m  0.2054\n",
      "      2        \u001b[36m3.6275\u001b[0m        \u001b[32m3.6759\u001b[0m  0.1961\n",
      "      3        \u001b[36m3.4781\u001b[0m        \u001b[32m3.6044\u001b[0m  0.2420\n",
      "      4        \u001b[36m3.3436\u001b[0m        \u001b[32m3.4350\u001b[0m  0.1696\n",
      "      5        \u001b[36m3.2237\u001b[0m        \u001b[32m3.3307\u001b[0m  0.1785\n",
      "      6        \u001b[36m3.1189\u001b[0m        \u001b[32m3.1517\u001b[0m  0.2114\n",
      "      7        \u001b[36m3.0277\u001b[0m        \u001b[32m3.0789\u001b[0m  0.2601\n",
      "      8        \u001b[36m2.9495\u001b[0m        \u001b[32m3.0286\u001b[0m  0.2189\n",
      "      9        \u001b[36m2.8850\u001b[0m        \u001b[32m2.9414\u001b[0m  0.1749\n",
      "     10        \u001b[36m2.8286\u001b[0m        2.9624  0.2108\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5836\u001b[0m        \u001b[32m3.6442\u001b[0m  0.1691\n",
      "      2        \u001b[36m3.4241\u001b[0m        \u001b[32m3.4936\u001b[0m  0.2155\n",
      "      3        \u001b[36m3.2934\u001b[0m        \u001b[32m3.3685\u001b[0m  0.2085\n",
      "      4        \u001b[36m3.1822\u001b[0m        \u001b[32m3.2620\u001b[0m  0.2195\n",
      "      5        \u001b[36m3.0864\u001b[0m        \u001b[32m3.1709\u001b[0m  0.1776\n",
      "      6        \u001b[36m3.0046\u001b[0m        \u001b[32m3.0938\u001b[0m  0.3033\n",
      "      7        \u001b[36m2.9354\u001b[0m        \u001b[32m3.0290\u001b[0m  0.3614\n",
      "      8        \u001b[36m2.8776\u001b[0m        \u001b[32m2.9746\u001b[0m  0.1952\n",
      "      9        \u001b[36m2.8291\u001b[0m        \u001b[32m2.9299\u001b[0m  0.2728\n",
      "     10        \u001b[36m2.7889\u001b[0m        \u001b[32m2.8912\u001b[0m  0.2132\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7535\u001b[0m        \u001b[32m3.7500\u001b[0m  0.1889\n",
      "      2        \u001b[36m3.6003\u001b[0m        \u001b[32m3.6043\u001b[0m  0.2782\n",
      "      3        \u001b[36m3.4613\u001b[0m        \u001b[32m3.4704\u001b[0m  0.2000\n",
      "      4        \u001b[36m3.3361\u001b[0m        \u001b[32m3.3508\u001b[0m  0.1802\n",
      "      5        \u001b[36m3.2252\u001b[0m        \u001b[32m3.2462\u001b[0m  0.2068\n",
      "      6        \u001b[36m3.1284\u001b[0m        \u001b[32m3.1559\u001b[0m  0.1848\n",
      "      7        \u001b[36m3.0441\u001b[0m        \u001b[32m3.0783\u001b[0m  0.1993\n",
      "      8        \u001b[36m2.9719\u001b[0m        \u001b[32m3.0109\u001b[0m  0.1838\n",
      "      9        \u001b[36m2.9102\u001b[0m        \u001b[32m2.9560\u001b[0m  0.2181\n",
      "     10        \u001b[36m2.8579\u001b[0m        \u001b[32m2.9083\u001b[0m  0.1813\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6604\u001b[0m        \u001b[32m3.7218\u001b[0m  0.2130\n",
      "      2        \u001b[36m3.5226\u001b[0m        \u001b[32m3.5887\u001b[0m  0.1974\n",
      "      3        \u001b[36m3.3971\u001b[0m        \u001b[32m3.4673\u001b[0m  0.1841\n",
      "      4        \u001b[36m3.2798\u001b[0m        \u001b[32m3.3542\u001b[0m  0.2222\n",
      "      5        \u001b[36m3.1700\u001b[0m        \u001b[32m3.2504\u001b[0m  0.3583\n",
      "      6        \u001b[36m3.0721\u001b[0m        \u001b[32m3.1592\u001b[0m  0.3036\n",
      "      7        \u001b[36m2.9883\u001b[0m        \u001b[32m3.0809\u001b[0m  0.1754\n",
      "      8        \u001b[36m2.9169\u001b[0m        \u001b[32m3.0136\u001b[0m  0.2235\n",
      "      9        \u001b[36m2.8560\u001b[0m        \u001b[32m2.9561\u001b[0m  0.2361\n",
      "     10        \u001b[36m2.8043\u001b[0m        \u001b[32m2.9077\u001b[0m  0.2012\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5933\u001b[0m        \u001b[32m3.3689\u001b[0m  0.1861\n",
      "      2        \u001b[36m3.3389\u001b[0m        \u001b[32m3.3085\u001b[0m  0.2033\n",
      "      3        \u001b[36m3.1396\u001b[0m        \u001b[32m3.1066\u001b[0m  0.2509\n",
      "      4        \u001b[36m2.9839\u001b[0m        \u001b[32m2.8696\u001b[0m  0.1736\n",
      "      5        \u001b[36m2.8686\u001b[0m        \u001b[32m2.7962\u001b[0m  0.1980\n",
      "      6        \u001b[36m2.7816\u001b[0m        \u001b[32m2.7102\u001b[0m  0.2290\n",
      "      7        \u001b[36m2.7175\u001b[0m        \u001b[32m2.6986\u001b[0m  0.1943\n",
      "      8        \u001b[36m2.6691\u001b[0m        \u001b[32m2.6516\u001b[0m  0.1858\n",
      "      9        \u001b[36m2.6329\u001b[0m        \u001b[32m2.6165\u001b[0m  0.2176\n",
      "     10        \u001b[36m2.6055\u001b[0m        \u001b[32m2.6083\u001b[0m  0.2050\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6005\u001b[0m        \u001b[32m3.5807\u001b[0m  0.2208\n",
      "      2        \u001b[36m3.3583\u001b[0m        \u001b[32m3.3609\u001b[0m  0.2497\n",
      "      3        \u001b[36m3.1615\u001b[0m        \u001b[32m3.1820\u001b[0m  0.1932\n",
      "      4        \u001b[36m3.0111\u001b[0m        \u001b[32m3.0504\u001b[0m  0.2237\n",
      "      5        \u001b[36m2.8938\u001b[0m        \u001b[32m2.9475\u001b[0m  0.2783\n",
      "      6        \u001b[36m2.8040\u001b[0m        \u001b[32m2.8669\u001b[0m  0.2408\n",
      "      7        \u001b[36m2.7358\u001b[0m        \u001b[32m2.8127\u001b[0m  0.1819\n",
      "      8        \u001b[36m2.6851\u001b[0m        \u001b[32m2.7155\u001b[0m  0.1863\n",
      "      9        \u001b[36m2.6480\u001b[0m        2.7256  0.2310\n",
      "     10        \u001b[36m2.6200\u001b[0m        \u001b[32m2.7070\u001b[0m  0.2006\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6391\u001b[0m        \u001b[32m3.5668\u001b[0m  0.2083\n",
      "      2        \u001b[36m3.3618\u001b[0m        \u001b[32m3.2252\u001b[0m  0.1992\n",
      "      3        \u001b[36m3.1401\u001b[0m        \u001b[32m3.1270\u001b[0m  0.1804\n",
      "      4        \u001b[36m2.9721\u001b[0m        \u001b[32m2.9787\u001b[0m  0.2329\n",
      "      5        \u001b[36m2.8500\u001b[0m        \u001b[32m2.9334\u001b[0m  0.1980\n",
      "      6        \u001b[36m2.7630\u001b[0m        \u001b[32m2.8145\u001b[0m  0.1955\n",
      "      7        \u001b[36m2.7031\u001b[0m        \u001b[32m2.7934\u001b[0m  0.2206\n",
      "      8        \u001b[36m2.6606\u001b[0m        \u001b[32m2.7601\u001b[0m  0.2215\n",
      "      9        \u001b[36m2.6296\u001b[0m        \u001b[32m2.7253\u001b[0m  0.2924\n",
      "     10        \u001b[36m2.6070\u001b[0m        \u001b[32m2.7026\u001b[0m  0.2364\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6243\u001b[0m        \u001b[32m3.5637\u001b[0m  0.2165\n",
      "      2        \u001b[36m3.3850\u001b[0m        \u001b[32m3.3690\u001b[0m  0.2302\n",
      "      3        \u001b[36m3.1909\u001b[0m        \u001b[32m3.1724\u001b[0m  0.1842\n",
      "      4        \u001b[36m3.0371\u001b[0m        \u001b[32m3.0592\u001b[0m  0.2422\n",
      "      5        \u001b[36m2.9210\u001b[0m        \u001b[32m2.9271\u001b[0m  0.1962\n",
      "      6        \u001b[36m2.8343\u001b[0m        \u001b[32m2.8831\u001b[0m  0.2482\n",
      "      7        \u001b[36m2.7677\u001b[0m        \u001b[32m2.8143\u001b[0m  0.2404\n",
      "      8        \u001b[36m2.7156\u001b[0m        \u001b[32m2.7335\u001b[0m  0.1999\n",
      "      9        \u001b[36m2.6770\u001b[0m        \u001b[32m2.7258\u001b[0m  0.2136\n",
      "     10        \u001b[36m2.6477\u001b[0m        2.7544  0.2425\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5962\u001b[0m        \u001b[32m3.5878\u001b[0m  0.1803\n",
      "      2        \u001b[36m3.3472\u001b[0m        \u001b[32m3.3514\u001b[0m  0.2337\n",
      "      3        \u001b[36m3.1481\u001b[0m        \u001b[32m3.1875\u001b[0m  0.2233\n",
      "      4        \u001b[36m2.9925\u001b[0m        \u001b[32m3.0329\u001b[0m  0.2156\n",
      "      5        \u001b[36m2.8768\u001b[0m        \u001b[32m2.9808\u001b[0m  0.1957\n",
      "      6        \u001b[36m2.7905\u001b[0m        \u001b[32m2.8786\u001b[0m  0.2003\n",
      "      7        \u001b[36m2.7274\u001b[0m        \u001b[32m2.8077\u001b[0m  0.1766\n",
      "      8        \u001b[36m2.6787\u001b[0m        \u001b[32m2.7718\u001b[0m  0.1955\n",
      "      9        \u001b[36m2.6424\u001b[0m        \u001b[32m2.7305\u001b[0m  0.2359\n",
      "     10        \u001b[36m2.6160\u001b[0m        \u001b[32m2.7107\u001b[0m  0.2173\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.8052\u001b[0m        \u001b[32m3.9480\u001b[0m  0.2216\n",
      "      2        \u001b[36m3.7765\u001b[0m        4.0073  0.1950\n",
      "      3        \u001b[36m3.7551\u001b[0m        \u001b[32m3.8494\u001b[0m  0.1760\n",
      "      4        \u001b[36m3.7353\u001b[0m        \u001b[32m3.8013\u001b[0m  0.2418\n",
      "      5        \u001b[36m3.7153\u001b[0m        3.8695  0.2221\n",
      "      6        \u001b[36m3.6952\u001b[0m        3.8441  0.2076\n",
      "      7        \u001b[36m3.6718\u001b[0m        3.8430  0.2459\n",
      "      8        \u001b[36m3.6541\u001b[0m        \u001b[32m3.7693\u001b[0m  0.2374\n",
      "      9        \u001b[36m3.6328\u001b[0m        \u001b[32m3.6642\u001b[0m  0.1820\n",
      "     10        \u001b[36m3.6132\u001b[0m        3.7469  0.2027\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6985\u001b[0m        \u001b[32m3.7934\u001b[0m  0.2037\n",
      "      2        \u001b[36m3.6781\u001b[0m        \u001b[32m3.7761\u001b[0m  0.2040\n",
      "      3        \u001b[36m3.6586\u001b[0m        \u001b[32m3.7570\u001b[0m  0.2036\n",
      "      4        \u001b[36m3.6389\u001b[0m        \u001b[32m3.7365\u001b[0m  0.2131\n",
      "      5        \u001b[36m3.6196\u001b[0m        \u001b[32m3.7206\u001b[0m  0.1968\n",
      "      6        \u001b[36m3.6018\u001b[0m        \u001b[32m3.7028\u001b[0m  0.2313\n",
      "      7        \u001b[36m3.5845\u001b[0m        \u001b[32m3.6836\u001b[0m  0.1815\n",
      "      8        \u001b[36m3.5672\u001b[0m        \u001b[32m3.6711\u001b[0m  0.2352\n",
      "      9        \u001b[36m3.5499\u001b[0m        \u001b[32m3.6598\u001b[0m  0.1854\n",
      "     10        \u001b[36m3.5334\u001b[0m        \u001b[32m3.6459\u001b[0m  0.2211\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7444\u001b[0m        \u001b[32m3.8749\u001b[0m  0.1984\n",
      "      2        \u001b[36m3.7226\u001b[0m        \u001b[32m3.8562\u001b[0m  0.2857\n",
      "      3        \u001b[36m3.7002\u001b[0m        \u001b[32m3.8365\u001b[0m  0.1896\n",
      "      4        \u001b[36m3.6781\u001b[0m        \u001b[32m3.8095\u001b[0m  0.2682\n",
      "      5        \u001b[36m3.6572\u001b[0m        \u001b[32m3.7895\u001b[0m  0.2303\n",
      "      6        \u001b[36m3.6368\u001b[0m        \u001b[32m3.7688\u001b[0m  0.2051\n",
      "      7        \u001b[36m3.6166\u001b[0m        \u001b[32m3.7487\u001b[0m  0.2423\n",
      "      8        \u001b[36m3.5967\u001b[0m        \u001b[32m3.7287\u001b[0m  0.2097\n",
      "      9        \u001b[36m3.5771\u001b[0m        \u001b[32m3.7083\u001b[0m  0.2463\n",
      "     10        \u001b[36m3.5578\u001b[0m        \u001b[32m3.6885\u001b[0m  0.2653\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.8813\u001b[0m        \u001b[32m4.0203\u001b[0m  0.2008\n",
      "      2        \u001b[36m3.8550\u001b[0m        \u001b[32m3.9484\u001b[0m  0.2323\n",
      "      3        \u001b[36m3.8290\u001b[0m        \u001b[32m3.9206\u001b[0m  0.1636\n",
      "      4        \u001b[36m3.8067\u001b[0m        \u001b[32m3.9024\u001b[0m  0.2585\n",
      "      5        \u001b[36m3.7856\u001b[0m        \u001b[32m3.8876\u001b[0m  0.1733\n",
      "      6        \u001b[36m3.7646\u001b[0m        \u001b[32m3.8730\u001b[0m  0.1890\n",
      "      7        \u001b[36m3.7438\u001b[0m        \u001b[32m3.8579\u001b[0m  0.1741\n",
      "      8        \u001b[36m3.7232\u001b[0m        \u001b[32m3.8430\u001b[0m  0.2069\n",
      "      9        \u001b[36m3.7028\u001b[0m        \u001b[32m3.8294\u001b[0m  0.1863\n",
      "     10        \u001b[36m3.6824\u001b[0m        \u001b[32m3.8159\u001b[0m  0.1844\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.8003\u001b[0m        \u001b[32m3.9419\u001b[0m  0.1753\n",
      "      2        \u001b[36m3.7670\u001b[0m        \u001b[32m3.9109\u001b[0m  0.1457\n",
      "      3        \u001b[36m3.7380\u001b[0m        \u001b[32m3.8813\u001b[0m  0.1456\n",
      "      4        \u001b[36m3.7123\u001b[0m        \u001b[32m3.8546\u001b[0m  0.1474\n",
      "      5        \u001b[36m3.6883\u001b[0m        \u001b[32m3.8290\u001b[0m  0.2110\n",
      "      6        \u001b[36m3.6656\u001b[0m        \u001b[32m3.8040\u001b[0m  0.1688\n",
      "      7        \u001b[36m3.6436\u001b[0m        \u001b[32m3.7801\u001b[0m  0.1869\n",
      "      8        \u001b[36m3.6221\u001b[0m        \u001b[32m3.7565\u001b[0m  0.2300\n",
      "      9        \u001b[36m3.6011\u001b[0m        \u001b[32m3.7342\u001b[0m  0.2891\n",
      "     10        \u001b[36m3.5806\u001b[0m        \u001b[32m3.7121\u001b[0m  0.2076\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.8327\u001b[0m        \u001b[32m3.7370\u001b[0m  0.2167\n",
      "      2        \u001b[36m3.5457\u001b[0m        \u001b[32m3.4572\u001b[0m  0.1849\n",
      "      3        \u001b[36m3.2989\u001b[0m        \u001b[32m3.2259\u001b[0m  0.2129\n",
      "      4        \u001b[36m3.0965\u001b[0m        \u001b[32m3.0430\u001b[0m  0.2307\n",
      "      5        \u001b[36m2.9436\u001b[0m        \u001b[32m2.9079\u001b[0m  0.2197\n",
      "      6        \u001b[36m2.8305\u001b[0m        \u001b[32m2.8092\u001b[0m  0.1990\n",
      "      7        \u001b[36m2.7485\u001b[0m        \u001b[32m2.7378\u001b[0m  0.1664\n",
      "      8        \u001b[36m2.6888\u001b[0m        \u001b[32m2.6857\u001b[0m  0.1712\n",
      "      9        \u001b[36m2.6456\u001b[0m        \u001b[32m2.6477\u001b[0m  0.2042\n",
      "     10        \u001b[36m2.6134\u001b[0m        \u001b[32m2.6193\u001b[0m  0.1769\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4906\u001b[0m        \u001b[32m3.3941\u001b[0m  0.1686\n",
      "      2        \u001b[36m3.2816\u001b[0m        \u001b[32m3.2978\u001b[0m  0.1854\n",
      "      3        \u001b[36m3.1185\u001b[0m        \u001b[32m3.1012\u001b[0m  0.1638\n",
      "      4        \u001b[36m2.9914\u001b[0m        \u001b[32m3.0204\u001b[0m  0.1982\n",
      "      5        \u001b[36m2.9009\u001b[0m        \u001b[32m2.9619\u001b[0m  0.2141\n",
      "      6        \u001b[36m2.8282\u001b[0m        \u001b[32m2.8887\u001b[0m  0.1765\n",
      "      7        \u001b[36m2.7690\u001b[0m        \u001b[32m2.8344\u001b[0m  0.1950\n",
      "      8        \u001b[36m2.7223\u001b[0m        \u001b[32m2.7892\u001b[0m  0.1756\n",
      "      9        \u001b[36m2.6880\u001b[0m        \u001b[32m2.7593\u001b[0m  0.1611\n",
      "     10        \u001b[36m2.6608\u001b[0m        \u001b[32m2.7336\u001b[0m  0.1630\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6244\u001b[0m        \u001b[32m3.7167\u001b[0m  0.1438\n",
      "      2        \u001b[36m3.3666\u001b[0m        \u001b[32m3.3627\u001b[0m  0.1535\n",
      "      3        \u001b[36m3.1585\u001b[0m        \u001b[32m3.2223\u001b[0m  0.1713\n",
      "      4        \u001b[36m2.9980\u001b[0m        \u001b[32m3.0391\u001b[0m  0.1560\n",
      "      5        \u001b[36m2.8811\u001b[0m        \u001b[32m2.9572\u001b[0m  0.1869\n",
      "      6        \u001b[36m2.7945\u001b[0m        \u001b[32m2.9002\u001b[0m  0.1669\n",
      "      7        \u001b[36m2.7314\u001b[0m        \u001b[32m2.8288\u001b[0m  0.1936\n",
      "      8        \u001b[36m2.6835\u001b[0m        \u001b[32m2.7748\u001b[0m  0.2303\n",
      "      9        \u001b[36m2.6484\u001b[0m        \u001b[32m2.7370\u001b[0m  0.1977\n",
      "     10        \u001b[36m2.6207\u001b[0m        \u001b[32m2.7122\u001b[0m  0.1905\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.5699\u001b[0m        \u001b[32m3.5180\u001b[0m  0.1710\n",
      "      2        \u001b[36m3.3174\u001b[0m        \u001b[32m3.3054\u001b[0m  0.1909\n",
      "      3        \u001b[36m3.1424\u001b[0m        \u001b[32m3.1494\u001b[0m  0.1612\n",
      "      4        \u001b[36m3.0090\u001b[0m        \u001b[32m3.0305\u001b[0m  0.1696\n",
      "      5        \u001b[36m2.9069\u001b[0m        \u001b[32m2.9400\u001b[0m  0.1755\n",
      "      6        \u001b[36m2.8290\u001b[0m        \u001b[32m2.8711\u001b[0m  0.1645\n",
      "      7        \u001b[36m2.7695\u001b[0m        \u001b[32m2.8185\u001b[0m  0.1639\n",
      "      8        \u001b[36m2.7217\u001b[0m        \u001b[32m2.7749\u001b[0m  0.1624\n",
      "      9        \u001b[36m2.6849\u001b[0m        \u001b[32m2.7429\u001b[0m  0.1559\n",
      "     10        \u001b[36m2.6568\u001b[0m        \u001b[32m2.7177\u001b[0m  0.1579\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.4759\u001b[0m        \u001b[32m3.4708\u001b[0m  0.1675\n",
      "      2        \u001b[36m3.2391\u001b[0m        \u001b[32m3.2576\u001b[0m  0.1569\n",
      "      3        \u001b[36m3.0575\u001b[0m        \u001b[32m3.0971\u001b[0m  0.1809\n",
      "      4        \u001b[36m2.9215\u001b[0m        \u001b[32m2.9783\u001b[0m  0.1929\n",
      "      5        \u001b[36m2.8210\u001b[0m        \u001b[32m2.8913\u001b[0m  0.1522\n",
      "      6        \u001b[36m2.7472\u001b[0m        \u001b[32m2.8277\u001b[0m  0.2244\n",
      "      7        \u001b[36m2.6925\u001b[0m        \u001b[32m2.7804\u001b[0m  0.1830\n",
      "      8        \u001b[36m2.6512\u001b[0m        \u001b[32m2.7447\u001b[0m  0.2108\n",
      "      9        \u001b[36m2.6198\u001b[0m        \u001b[32m2.7175\u001b[0m  0.1833\n",
      "     10        \u001b[36m2.5954\u001b[0m        \u001b[32m2.6962\u001b[0m  0.2144\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7381\u001b[0m        \u001b[32m3.8026\u001b[0m  0.2519\n",
      "      2        \u001b[36m3.5982\u001b[0m        \u001b[32m3.7905\u001b[0m  0.2117\n",
      "      3        \u001b[36m3.4685\u001b[0m        \u001b[32m3.4539\u001b[0m  0.2194\n",
      "      4        \u001b[36m3.3509\u001b[0m        \u001b[32m3.3059\u001b[0m  0.2084\n",
      "      5        \u001b[36m3.2450\u001b[0m        \u001b[32m3.2806\u001b[0m  0.1998\n",
      "      6        \u001b[36m3.1491\u001b[0m        \u001b[32m3.0783\u001b[0m  0.1786\n",
      "      7        \u001b[36m3.0643\u001b[0m        3.0837  0.1878\n",
      "      8        \u001b[36m2.9901\u001b[0m        \u001b[32m3.0288\u001b[0m  0.1584\n",
      "      9        \u001b[36m2.9266\u001b[0m        \u001b[32m2.8805\u001b[0m  0.2182\n",
      "     10        \u001b[36m2.8707\u001b[0m        \u001b[32m2.8471\u001b[0m  0.2204\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6613\u001b[0m        \u001b[32m3.7080\u001b[0m  0.2347\n",
      "      2        \u001b[36m3.5228\u001b[0m        \u001b[32m3.5760\u001b[0m  0.1691\n",
      "      3        \u001b[36m3.3968\u001b[0m        \u001b[32m3.4556\u001b[0m  0.2006\n",
      "      4        \u001b[36m3.2844\u001b[0m        \u001b[32m3.3483\u001b[0m  0.2146\n",
      "      5        \u001b[36m3.1850\u001b[0m        \u001b[32m3.2529\u001b[0m  0.1759\n",
      "      6        \u001b[36m3.0974\u001b[0m        \u001b[32m3.1689\u001b[0m  0.1750\n",
      "      7        \u001b[36m3.0207\u001b[0m        \u001b[32m3.0951\u001b[0m  0.2057\n",
      "      8        \u001b[36m2.9541\u001b[0m        \u001b[32m3.0325\u001b[0m  0.2432\n",
      "      9        \u001b[36m2.8964\u001b[0m        \u001b[32m2.9792\u001b[0m  0.2557\n",
      "     10        \u001b[36m2.8468\u001b[0m        \u001b[32m2.9322\u001b[0m  0.1938\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7225\u001b[0m        \u001b[32m3.7585\u001b[0m  0.2192\n",
      "      2        \u001b[36m3.5879\u001b[0m        \u001b[32m3.6267\u001b[0m  0.2140\n",
      "      3        \u001b[36m3.4646\u001b[0m        \u001b[32m3.4349\u001b[0m  0.2566\n",
      "      4        \u001b[36m3.3436\u001b[0m        \u001b[32m3.4001\u001b[0m  0.2228\n",
      "      5        \u001b[36m3.2298\u001b[0m        \u001b[32m3.1631\u001b[0m  0.2420\n",
      "      6        \u001b[36m3.1270\u001b[0m        3.1870  0.2421\n",
      "      7        \u001b[36m3.0375\u001b[0m        \u001b[32m3.1163\u001b[0m  0.1762\n",
      "      8        \u001b[36m2.9630\u001b[0m        \u001b[32m3.0443\u001b[0m  0.2031\n",
      "      9        \u001b[36m2.8993\u001b[0m        \u001b[32m2.9850\u001b[0m  0.2608\n",
      "     10        \u001b[36m2.8454\u001b[0m        \u001b[32m2.9321\u001b[0m  0.2208\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.6735\u001b[0m        \u001b[32m3.6752\u001b[0m  0.2282\n",
      "      2        \u001b[36m3.5284\u001b[0m        \u001b[32m3.5284\u001b[0m  0.2592\n",
      "      3        \u001b[36m3.4012\u001b[0m        \u001b[32m3.3869\u001b[0m  0.2851\n",
      "      4        \u001b[36m3.2869\u001b[0m        \u001b[32m3.3429\u001b[0m  0.2216\n",
      "      5        \u001b[36m3.1867\u001b[0m        \u001b[32m3.2886\u001b[0m  0.1957\n",
      "      6        \u001b[36m3.1012\u001b[0m        \u001b[32m3.0601\u001b[0m  0.1973\n",
      "      7        \u001b[36m3.0261\u001b[0m        \u001b[32m3.0242\u001b[0m  0.2310\n",
      "      8        \u001b[36m2.9616\u001b[0m        3.0314  0.2139\n",
      "      9        \u001b[36m2.9071\u001b[0m        \u001b[32m2.9162\u001b[0m  0.2034\n",
      "     10        \u001b[36m2.8622\u001b[0m        \u001b[32m2.9115\u001b[0m  0.2509\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m3.7757\u001b[0m        \u001b[32m3.8296\u001b[0m  0.1554\n",
      "      2        \u001b[36m3.6262\u001b[0m        \u001b[32m3.6898\u001b[0m  0.1699\n",
      "      3        \u001b[36m3.4859\u001b[0m        \u001b[32m3.5570\u001b[0m  0.2364\n",
      "      4        \u001b[36m3.3590\u001b[0m        \u001b[32m3.4421\u001b[0m  0.2941\n",
      "      5        \u001b[36m3.2405\u001b[0m        \u001b[32m3.2959\u001b[0m  0.1802\n",
      "      6        \u001b[36m3.1362\u001b[0m        \u001b[32m3.2031\u001b[0m  0.2355\n",
      "      7        \u001b[36m3.0460\u001b[0m        \u001b[32m3.1364\u001b[0m  0.2426\n",
      "      8        \u001b[36m2.9678\u001b[0m        \u001b[32m3.0495\u001b[0m  0.2237\n",
      "      9        \u001b[36m2.9008\u001b[0m        \u001b[32m3.0046\u001b[0m  0.2537\n",
      "     10        \u001b[36m2.8436\u001b[0m        \u001b[32m2.9493\u001b[0m  0.1834\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(estimator=&lt;class &#x27;skorch.net.NeuralNet&#x27;&gt;[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)...\n",
       "                                        &#x27;module__n_neurons&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])},\n",
       "                   refit=False, scoring=&#x27;neg_mean_squared_error&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(estimator=&lt;class &#x27;skorch.net.NeuralNet&#x27;&gt;[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)...\n",
       "                                        &#x27;module__n_neurons&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])},\n",
       "                   refit=False, scoring=&#x27;neg_mean_squared_error&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: NeuralNet</label><div class=\"sk-toggleable__content\"><pre>&lt;class &#x27;skorch.net.NeuralNet&#x27;&gt;[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)\n",
       "    (act2): Sigmoid()\n",
       "  ),\n",
       ")</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NeuralNet</label><div class=\"sk-toggleable__content\"><pre>&lt;class &#x27;skorch.net.NeuralNet&#x27;&gt;[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)\n",
       "    (act2): Sigmoid()\n",
       "  ),\n",
       ")</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(estimator=<class 'skorch.net.NeuralNet'>[initialized](\n",
       "  module_=MyNeuralNetwork(\n",
       "    (linear1): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (normalize1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.3, inplace=False)\n",
       "    (act1): ReLU()\n",
       "    (linear2): Linear(in_features=50, out_features=1, bias=True)...\n",
       "                                        'module__n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])},\n",
       "                   refit=False, scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnd_search_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b5799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.016097753230597586,\n",
       " 'module__X': tensor([[   3.2596,   33.0000,    5.0177,  ...,    3.6918,   32.7100,\n",
       "          -117.0300],\n",
       "         [   3.8125,   49.0000,    4.4735,  ...,    1.7381,   33.7700,\n",
       "          -118.1600],\n",
       "         [   4.1563,    4.0000,    5.6458,  ...,    2.7232,   34.6600,\n",
       "          -120.4800],\n",
       "         ...,\n",
       "         [   2.9344,   36.0000,    3.9867,  ...,    3.3321,   34.0300,\n",
       "          -118.3800],\n",
       "         [   5.7192,   15.0000,    6.3953,  ...,    3.1789,   37.5800,\n",
       "          -121.9600],\n",
       "         [   2.5755,   52.0000,    3.4026,  ...,    2.1087,   37.7700,\n",
       "          -122.4200]]),\n",
       " 'module__n_neurons': 69}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ae8aa5",
   "metadata": {},
   "source": [
    "However, this approach is quite time consuming, thus, allows you to test only a few combinations of hyperparameters. Some suggestions:\n",
    "- It is better to have too many layers, and use regularization\n",
    "- Try between 1 and 5 layers\n",
    "- Adding layers usually helps more than adding neurons\n",
    "- Using ReLU in hidden layers is a good baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.398px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
